{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10328861,"sourceType":"datasetVersion","datasetId":6395471},{"sourceId":10349036,"sourceType":"datasetVersion","datasetId":6296913},{"sourceId":10358109,"sourceType":"datasetVersion","datasetId":5987034},{"sourceId":10391050,"sourceType":"datasetVersion","datasetId":5684885},{"sourceId":10550838,"sourceType":"datasetVersion","datasetId":6411015}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"id":"exmY-abTefFW"}},{"cell_type":"code","source":"!pip install ffmpeg-python\n!pip install av\n!pip install -q -U google-generativeai\n!pip install --upgrade pip\n!pip install --upgrade transformers datasets[audio] accelerate\n!pip install scenedetect","metadata":{"id":"Bi3c0wLhh-Rf","outputId":"44c463e5-7c4c-45cf-daca-75620249739c","trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:27:47.899803Z","iopub.execute_input":"2025-01-06T07:27:47.900243Z","iopub.status.idle":"2025-01-06T07:28:24.097865Z","shell.execute_reply.started":"2025-01-06T07:27:47.900197Z","shell.execute_reply":"2025-01-06T07:28:24.096320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport time\nimport torch\nimport random\nimport ffmpeg\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom glob import glob\nimport soundfile as sf\nfrom json import loads,dumps\n#from pydub import AudioSegment\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom scipy.signal import resample\nimport typing_extensions as typing\nfrom google.generativeai.types import HarmCategory, HarmBlockThreshold\nfrom scenedetect import open_video, VideoStreamCv2, SceneManager\nfrom scenedetect.detectors import ContentDetector","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:28:24.100571Z","iopub.execute_input":"2025-01-06T07:28:24.100997Z","iopub.status.idle":"2025-01-06T07:28:24.940863Z","shell.execute_reply.started":"2025-01-06T07:28:24.100955Z","shell.execute_reply":"2025-01-06T07:28:24.939049Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"# Extracting all required ids \n\nlabels_df = pd.read_csv('/kaggle/input/youtube-data/all_unique_codes3.csv') \ntranscriptions_df = pd.read_csv('/kaggle/input/youtube-data/Translated-transcriptions.csv')\n\n# Merging on different key names\ndf = pd.merge(labels_df, transcriptions_df, left_on='Video link', right_on='Video Id')\ndf ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:30:36.861967Z","iopub.execute_input":"2025-01-06T07:30:36.862452Z","iopub.status.idle":"2025-01-06T07:30:37.047660Z","shell.execute_reply.started":"2025-01-06T07:30:36.862414Z","shell.execute_reply":"2025-01-06T07:30:37.046367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extracting video ids and primary labels\n\nvideo_ids = list(df['Video Id'])\nprimary_labels = list(df['Primary Tag'])\nall_transcriptions = list(df['Transcription'])","metadata":{"id":"oR7rNEX3rl2I","trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:30:37.099467Z","iopub.execute_input":"2025-01-06T07:30:37.100475Z","iopub.status.idle":"2025-01-06T07:30:37.106497Z","shell.execute_reply.started":"2025-01-06T07:30:37.100434Z","shell.execute_reply":"2025-01-06T07:30:37.105362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(video_ids) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:30:37.689681Z","iopub.execute_input":"2025-01-06T07:30:37.690053Z","iopub.status.idle":"2025-01-06T07:30:37.697681Z","shell.execute_reply.started":"2025-01-06T07:30:37.690022Z","shell.execute_reply":"2025-01-06T07:30:37.696526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extracting data from transcripts\n\ntranscriptions = []\nlengths = []\n\nfor (i, id_) in enumerate(video_ids):\n    transcriptions.append(all_transcriptions[i].split(\"chunks\")[0])\n    lengths.append(len(all_transcriptions[i].split(\"chunks\")[0]))","metadata":{"id":"_7p-efT9sNMf","trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:30:38.687106Z","iopub.execute_input":"2025-01-06T07:30:38.687570Z","iopub.status.idle":"2025-01-06T07:30:38.704652Z","shell.execute_reply.started":"2025-01-06T07:30:38.687535Z","shell.execute_reply":"2025-01-06T07:30:38.703390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"available_ad_ids = os.listdir('/kaggle/input/youtube-data/Ads/Ads') \nlen(available_ad_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:30:39.693085Z","iopub.execute_input":"2025-01-06T07:30:39.693538Z","iopub.status.idle":"2025-01-06T07:30:39.703931Z","shell.execute_reply.started":"2025-01-06T07:30:39.693502Z","shell.execute_reply":"2025-01-06T07:30:39.702461Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Extracting Images** ","metadata":{}},{"cell_type":"code","source":"def detect_scenes(video_path, threshold = 30):\n    \"\"\"Detect scenes in a video and return scene start and end frames.\"\"\"\n    scene_list = []\n    while len(scene_list) < 6 and threshold > 0:\n        threshold //= 2\n    \n        video = open_video(video_path)\n        scene_manager = SceneManager()\n        scene_manager.add_detector(ContentDetector(threshold=threshold))\n    \n        scene_manager.detect_scenes(video)\n        scene_list = scene_manager.get_scene_list()\n    \n    return scene_list\n\n\ndef get_top_n_longest_scenes(scene_list, n):\n    '''Return the top n longest scenes with start and end frame indices.'''\n    scene_durations = [(start, end - start) for start, end in scene_list]\n    scene_durations.sort(key=lambda x: x[1], reverse=True)\n\n    # Top n longest scenes with start and end frame indices\n    longest_scenes = [(start, start + duration) for start, duration in scene_durations[:n]]\n    return longest_scenes\n\n\ndef sort_scenes_by_frame(scenes_list):\n    '''Sort scenes by their start frame number.'''\n    sorted_scenes = sorted(scenes_list, key=lambda scene: scene[0].get_frames())\n    return sorted_scenes\n\n\ndef get_num_grids(video_path):\n    '''Get number of grids to be created'''\n    cap = cv2.VideoCapture(video_path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    duration = total_frames / fps\n\n    # Calculate number of grids based on the duration\n    duration = round(duration, 2)\n    if ((duration // 60) + 1) <= 5:\n        return int(((duration // 60) + 1))\n    else:\n        return 5\n\n\ndef extract_k_frames_from_scene(video_path, scene, k):\n    '''Extract k frames evenly spaced from each scene.'''\n    # Extract frame numbers from scene start and end\n    start_frame = scene[0].get_frames() + 1\n    end_frame = scene[1].get_frames() - 1\n\n    # Create k equally spaced frame indices within the scene's range\n    frame_indices = np.linspace(start_frame, end_frame, k, dtype=int)\n    \n    cap = cv2.VideoCapture(video_path)\n    frames = []\n\n    # Extract frames from calculated indices\n    for frame_no in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n        ret, frame = cap.read()\n        if ret:\n            frames.append(frame)\n    \n    cap.release()\n    return frames\n\n\ndef create_image_grid(frames, grid_size=(1000, 1000)):\n    '''Arrange 6 frames into a 3x2 grid and resize to the specified grid size.'''\n    # Ensure all frames have the same size for concatenation\n    frames = [cv2.resize(frame, (640, 360)) for frame in frames]  # Resize to a common size like 640x360\n    rows = [np.concatenate(frames[i:i+2], axis=1) for i in range(0, 6, 2)]\n    image_grid = np.concatenate(rows, axis=0)\n    \n    return np.array(Image.fromarray(image_grid).resize(grid_size))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:30:40.879200Z","iopub.execute_input":"2025-01-06T07:30:40.879627Z","iopub.status.idle":"2025-01-06T07:30:41.174741Z","shell.execute_reply.started":"2025-01-06T07:30:40.879590Z","shell.execute_reply":"2025-01-06T07:30:41.173554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_images(video_path, n=6):\n    ''' 1. Detect scenes\n        2. Get k; where k = num_grids\n        3. Get the 6 longest scenes\n        4. Sort scenes wrt frame numbers\n        5. Extract n * k frames\n        6. Create k image grids of n frames each\n     '''\n    scene_list = detect_scenes(video_path)\n    k = get_num_grids(video_path)\n    longest_scenes = get_top_n_longest_scenes(scene_list, n*k)\n    scenes = sort_scenes_by_frame(longest_scenes)\n\n    frames = []\n    for scene in scenes:\n        frames.extend(extract_k_frames_from_scene(video_path, scene, 1))\n\n    grids = []\n    for i in range(k):\n        start_idx = i * n\n        end_idx = start_idx + n\n        grid_frames = frames[start_idx:end_idx]\n        grid = create_image_grid(grid_frames, grid_size=(1000, 1000))\n        grids.append(grid)\n\n    return grids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:30:42.393424Z","iopub.execute_input":"2025-01-06T07:30:42.393857Z","iopub.status.idle":"2025-01-06T07:30:42.401622Z","shell.execute_reply.started":"2025-01-06T07:30:42.393824Z","shell.execute_reply":"2025-01-06T07:30:42.400435Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Defining Model** ","metadata":{}},{"cell_type":"code","source":"key_ = '' # add key here ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:30:44.374654Z","iopub.execute_input":"2025-01-06T07:30:44.375067Z","iopub.status.idle":"2025-01-06T07:30:44.383705Z","shell.execute_reply.started":"2025-01-06T07:30:44.375031Z","shell.execute_reply":"2025-01-06T07:30:44.382392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import google.generativeai as genai \nos.environ[\"API_KEY\"] = key_ \ngenai.configure(api_key=os.environ[\"API_KEY\"]) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:30:46.097364Z","iopub.execute_input":"2025-01-06T07:30:46.097759Z","iopub.status.idle":"2025-01-06T07:30:46.103999Z","shell.execute_reply.started":"2025-01-06T07:30:46.097728Z","shell.execute_reply":"2025-01-06T07:30:46.102589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(torch.cuda.is_available())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:30:47.150734Z","iopub.execute_input":"2025-01-06T07:30:47.151151Z","iopub.status.idle":"2025-01-06T07:30:47.158172Z","shell.execute_reply.started":"2025-01-06T07:30:47.151109Z","shell.execute_reply":"2025-01-06T07:30:47.156797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = genai.GenerativeModel(\"gemini-1.5-flash\", system_instruction=\"You are an expert content moderator.\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:30:47.686280Z","iopub.execute_input":"2025-01-06T07:30:47.686709Z","iopub.status.idle":"2025-01-06T07:30:47.693543Z","shell.execute_reply.started":"2025-01-06T07:30:47.686674Z","shell.execute_reply":"2025-01-06T07:30:47.692227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LLM_Output(typing.TypedDict):\n    label: str\n    language: list[str]\n    response: str","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:30:48.972767Z","iopub.execute_input":"2025-01-06T07:30:48.973211Z","iopub.status.idle":"2025-01-06T07:30:48.979238Z","shell.execute_reply.started":"2025-01-06T07:30:48.973163Z","shell.execute_reply":"2025-01-06T07:30:48.977900Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Prompt** ","metadata":{}},{"cell_type":"code","source":"prompt =  \"\"\" A video is inappropriate for children if it contains physical violence (cartoonish, realistic, inclusion of gore),\n  interpersonal violence (such as bullying, pranks, meanness, humor at expense of others, belittling ,controlling, talking down to, or manipulating others),\n  self-harm or suicide,\n  elements of scariness (horror, scary themes, creepy or suspenseful themes, loud and violent content, zombies, skeletons, scary masks, scary clowns, blood, fire in dangerous settings,\n  mentally distressing scenes (car crashes, medical procedures etc),\n  extreme stunts (depictions of life endangering or high-risk physical activities/challenges),\n  sexual content (innuendos, overt sexual behavior, nudity, sexual themes, sexual suggestiveness),\n  discrimination (based on race/ethnicity/gender/ability),\n  offensive language (from mild words such as \"idiot\" and \"stupid\" to severe swearing, hate speech or signs),\n  drinking, drugs, smoking, gambling, crude humour,\n  inappropriate clothing (with significant skin exposure),\n  death,\n  intimacy (sensual physical contact, marriage, romance, dating and relationships, apparent romantic conduct between subjects/actors).\n  romantic songs and music.\n\n  You have been given an image grid (3 x 2) representing a video and it's audio transcription.\n  Consider that you are a Parent. Based on both the visual content and audio transcription, is the video ad provided inappropriate for children under 9 based on this definition?\n  If it is, label it as \"inappropriate\".\n  If it is not, label it as \"appropriate\".\n\n  Justify your conclusion.\n\n  Also, mention what language is being spoken if there is any.\n\n  Make the outputs in JSON format.\n\n  \"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:30:50.592248Z","iopub.execute_input":"2025-01-06T07:30:50.592677Z","iopub.status.idle":"2025-01-06T07:30:50.599519Z","shell.execute_reply.started":"2025-01-06T07:30:50.592639Z","shell.execute_reply":"2025-01-06T07:30:50.598373Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Running Model on Dataset** ","metadata":{"id":"w8esg7q8hSTW"}},{"cell_type":"code","source":"len(video_ids) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:30:52.703399Z","iopub.execute_input":"2025-01-06T07:30:52.703802Z","iopub.status.idle":"2025-01-06T07:30:52.711449Z","shell.execute_reply.started":"2025-01-06T07:30:52.703767Z","shell.execute_reply":"2025-01-06T07:30:52.710149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ids = []\npredicted_labels = []\nlanguages = []\nresponses = []\nground_truths = []\nremaining = []\n\nimg_dir = '/kaggle/working/Images'\nif not os.path.exists(img_dir):\n    os.makedirs(img_dir)\n    \nfor i in range(len(video_ids)): \n\n    if video_ids[i] in available_ad_ids:\n\n        try:\n            contents_of_ad = os.listdir('/kaggle/input/youtube-data/Ads/Ads/' + video_ids[i]) \n            contents_of_ad.remove('audio.mp3') \n            video_path = '/kaggle/input/youtube-data/Ads/Ads/' + video_ids[i] + '/' + contents_of_ad[0] \n\n            # Extract multiple images representative of the video\n            images = get_images(video_path) \n\n            # Save each image returned by extract_images_of_frames\n            image_paths = []\n            for idx, img in enumerate(images):\n                # Convert NumPy array to PIL image\n                image = Image.fromarray(img)\n                \n                # Save the image to a file\n                image_name = f\"{video_ids[i]}_{idx + 1}.png\"\n                image_path = os.path.join(img_dir, image_name)\n                image.save(image_path)\n                image_paths.append(image_path)\n                print(image_path)\n                \n            # Display the images\n            fig, axes = plt.subplots(1, len(images), figsize=(10, 3))\n            if len(images) == 1:\n                axes.imshow(images[0])\n                axes.axis('off')\n            else:\n                for ax, img in zip(axes, images):\n                    ax.imshow(img)\n                    ax.axis('off')\n\n            plt.tight_layout()\n            plt.show()\n\n            # Upload images and handle potential errors\n            uploaded_files = []\n            try:\n                for image_path in image_paths:\n                    uploaded_file = genai.upload_file(path=image_path, resumable=False)\n                    uploaded_files.append(uploaded_file)\n            except Exception as e:\n                print(f\"Error uploading images: {e}\")\n                remaining.append(video_ids[i])\n                continue\n\n            # Check if all images have uploaded\n            try:\n                for uploaded_file in uploaded_files:\n                    while uploaded_file.state.name == \"PROCESSING\":\n                        print('.', end='')\n                        time.sleep(10)\n                        uploaded_file = genai.get_file(uploaded_file.name)\n                    if uploaded_file.state.name == \"FAILED\":\n                        raise ValueError(uploaded_file.state.name)\n            except Exception as e:\n                print(f\"Error during image processing: {e}\")\n                remaining.append(video_ids[i])\n                continue\n\n            # Make inference with audio and image URIs\n            audio = transcriptions[i]\n            try: \n                inputs_ = [audio] \n                inputs_.extend(uploaded_files) \n                inputs_.extend([prompt]) \n                response = model.generate_content(inputs_,\n                                                  generation_config=genai.GenerationConfig(\n                                                      response_mime_type=\"application/json\",\n                                                      response_schema=LLM_Output, \n                                                      temperature=0.0), \n                                                  safety_settings={\n                                                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n                                                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n                                                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n                                                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n                                                  })\n            except Exception as e:\n                print(f\"Error making inference: {e}\")\n                remaining.append(video_ids[i])\n                continue\n            try:\n                print(\"Completed for video number:\", i, ' ', video_ids[i])\n\n                dictionary = loads(response.text)\n                print('True Label:', primary_labels[i], 'Response:', dictionary)\n\n                ids.append(video_ids[i])\n                predicted_labels.append(dictionary['label'])\n                languages.append(dictionary['language'])\n                responses.append(dictionary['response'])\n                ground_truths.append(primary_labels[i]) \n                \n            except Exception as e:\n                print(f\"Error processing response.text: {e}\")\n                remaining.append(video_ids[i])\n                continue\n\n        except Exception as e:\n            print(f\"Unexpected error: {e}\")\n            remaining.append(video_ids[i])\n            continue\n\n        time.sleep(20)","metadata":{"id":"cSoT7-9DDknR","trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:30:55.754774Z","iopub.execute_input":"2025-01-06T07:30:55.756122Z","iopub.status.idle":"2025-01-06T07:32:13.308748Z","shell.execute_reply.started":"2025-01-06T07:30:55.756049Z","shell.execute_reply":"2025-01-06T07:32:13.307507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:32:13.311094Z","iopub.execute_input":"2025-01-06T07:32:13.311612Z","iopub.status.idle":"2025-01-06T07:32:13.319369Z","shell.execute_reply.started":"2025-01-06T07:32:13.311561Z","shell.execute_reply":"2025-01-06T07:32:13.317996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# At the end, print remaining videos \n\nprint(\"Remaining videos with errors:\", remaining) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:32:13.320998Z","iopub.execute_input":"2025-01-06T07:32:13.321501Z","iopub.status.idle":"2025-01-06T07:32:13.336464Z","shell.execute_reply.started":"2025-01-06T07:32:13.321452Z","shell.execute_reply":"2025-01-06T07:32:13.334831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(responses)): \n    print('True Label: ', ground_truths[i], '\\tPrediction: ', predicted_labels[i]) ","metadata":{"id":"aqDlWLZcZJHL","trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:32:13.340260Z","iopub.execute_input":"2025-01-06T07:32:13.340723Z","iopub.status.idle":"2025-01-06T07:32:13.356907Z","shell.execute_reply.started":"2025-01-06T07:32:13.340687Z","shell.execute_reply":"2025-01-06T07:32:13.355560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df = pd.DataFrame({\n    'Video Id': ids,\n    'Primary Label': ground_truths,\n    'Predicted Label': predicted_labels,\n    'Response': responses, \n    'Languages': languages \n})\n\nnew_df.head() ","metadata":{"id":"6JsiW7gbvxRA","trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:32:13.358160Z","iopub.execute_input":"2025-01-06T07:32:13.358543Z","iopub.status.idle":"2025-01-06T07:32:13.379112Z","shell.execute_reply.started":"2025-01-06T07:32:13.358510Z","shell.execute_reply":"2025-01-06T07:32:13.377941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_dir = '/kaggle/working/results'\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:32:13.380516Z","iopub.execute_input":"2025-01-06T07:32:13.380851Z","iopub.status.idle":"2025-01-06T07:32:13.392513Z","shell.execute_reply.started":"2025-01-06T07:32:13.380820Z","shell.execute_reply":"2025-01-06T07:32:13.391338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df.to_csv('/kaggle/working/results/DAVSP_Gemini-1.5-Flash.csv', index=False)","metadata":{"id":"Tv1_5sfJwRfu","trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:32:13.394344Z","iopub.execute_input":"2025-01-06T07:32:13.394698Z","iopub.status.idle":"2025-01-06T07:32:13.409013Z","shell.execute_reply.started":"2025-01-06T07:32:13.394657Z","shell.execute_reply":"2025-01-06T07:32:13.407839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Changing to binary lists \n\npredictions = [1 if pred == 'inappropriate' else 0 for pred in predicted_labels] \nground_truths = [1 if label == 'inappropriate' else 0 for label in ground_truths] ","metadata":{"id":"4hsLj8P8Mg-e","trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:32:13.410450Z","iopub.execute_input":"2025-01-06T07:32:13.410759Z","iopub.status.idle":"2025-01-06T07:32:13.424684Z","shell.execute_reply.started":"2025-01-06T07:32:13.410730Z","shell.execute_reply":"2025-01-06T07:32:13.423414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Obtaining classification report \nfrom sklearn.metrics import classification_report \n\nreport = classification_report(ground_truths, predictions) \nprint(report) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:32:13.426313Z","iopub.execute_input":"2025-01-06T07:32:13.427319Z","iopub.status.idle":"2025-01-06T07:32:13.452741Z","shell.execute_reply.started":"2025-01-06T07:32:13.427175Z","shell.execute_reply":"2025-01-06T07:32:13.451538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(ground_truths, predictions)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Child Directed', 'Inapproriate'], yticklabels=['Child Directed', 'Inapproriate'])\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T07:32:13.456278Z","iopub.execute_input":"2025-01-06T07:32:13.456782Z","iopub.status.idle":"2025-01-06T07:32:13.744278Z","shell.execute_reply.started":"2025-01-06T07:32:13.456734Z","shell.execute_reply":"2025-01-06T07:32:13.743069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}