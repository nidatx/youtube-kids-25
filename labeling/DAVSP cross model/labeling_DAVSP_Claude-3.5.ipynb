{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10328861,"sourceType":"datasetVersion","datasetId":6395471},{"sourceId":10349036,"sourceType":"datasetVersion","datasetId":6296913},{"sourceId":10358109,"sourceType":"datasetVersion","datasetId":5987034},{"sourceId":10391050,"sourceType":"datasetVersion","datasetId":5684885},{"sourceId":10550838,"sourceType":"datasetVersion","datasetId":6411015}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Imports","metadata":{"id":"exmY-abTefFW"}},{"cell_type":"code","source":"!pip install anthropic","metadata":{"id":"Bi3c0wLhh-Rf","outputId":"44c463e5-7c4c-45cf-daca-75620249739c","execution":{"iopub.status.busy":"2024-12-30T10:22:54.561562Z","iopub.execute_input":"2024-12-30T10:22:54.562054Z","iopub.status.idle":"2024-12-30T10:23:07.813092Z","shell.execute_reply.started":"2024-12-30T10:22:54.562009Z","shell.execute_reply":"2024-12-30T10:23:07.811810Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport anthropic\nfrom json import loads,dumps\nimport time\nfrom glob import glob\nimport json\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2024-12-30T10:23:07.815869Z","iopub.execute_input":"2024-12-30T10:23:07.816359Z","iopub.status.idle":"2024-12-30T10:23:09.923771Z","shell.execute_reply.started":"2024-12-30T10:23:07.816308Z","shell.execute_reply":"2024-12-30T10:23:09.922688Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install ffmpeg-python","metadata":{"id":"oIKIB_gZD_bf","outputId":"f4ca6584-37c8-473f-bef7-6f4d6152a3f1","execution":{"iopub.status.busy":"2024-12-30T10:23:09.925067Z","iopub.execute_input":"2024-12-30T10:23:09.925600Z","iopub.status.idle":"2024-12-30T10:23:21.390093Z","shell.execute_reply.started":"2024-12-30T10:23:09.925554Z","shell.execute_reply":"2024-12-30T10:23:21.388531Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install av","metadata":{"id":"dJk_T8_vEH0H","outputId":"44fdc67e-5fba-4f8f-93e4-d6abf9e37be5","execution":{"iopub.status.busy":"2024-12-30T10:23:21.393855Z","iopub.execute_input":"2024-12-30T10:23:21.394393Z","iopub.status.idle":"2024-12-30T10:23:33.945622Z","shell.execute_reply.started":"2024-12-30T10:23:21.394337Z","shell.execute_reply":"2024-12-30T10:23:33.943976Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install scenedetect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:23:33.947203Z","iopub.execute_input":"2024-12-30T10:23:33.947581Z","iopub.status.idle":"2024-12-30T10:23:45.501655Z","shell.execute_reply.started":"2024-12-30T10:23:33.947543Z","shell.execute_reply":"2024-12-30T10:23:45.500330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport torch\nimport random\nimport ffmpeg\nimport warnings\nimport numpy as np\nfrom PIL import Image\nfrom glob import glob\nimport soundfile as sf\nimport matplotlib.pyplot as plt\n#from pydub import AudioSegment\nfrom scipy.signal import resample\nimport typing_extensions as typing\nfrom scenedetect import open_video,  VideoStreamCv2, SceneManager\nfrom scenedetect.detectors import ContentDetector","metadata":{"id":"iNg0gXOfD_hY","execution":{"iopub.status.busy":"2024-12-30T10:23:45.503500Z","iopub.execute_input":"2024-12-30T10:23:45.504018Z","iopub.status.idle":"2024-12-30T10:23:49.746885Z","shell.execute_reply.started":"2024-12-30T10:23:45.503965Z","shell.execute_reply":"2024-12-30T10:23:49.745627Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extracting all required ids \n\nlabels_df = pd.read_csv('/kaggle/input/youtube-data/all_unique_codes3.csv')\ntranscriptions_df = pd.read_csv('/kaggle/input/youtube-data/Translated-transcriptions.csv')\n\n# Merging on different key names\ndf = pd.merge(labels_df, transcriptions_df, left_on='Video link', right_on='Video Id')\ndf.head() ","metadata":{"id":"1ksUpR0ibKve","outputId":"6880ee21-f703-4761-dfcd-842e1f8b61d9","execution":{"iopub.status.busy":"2024-12-30T10:23:49.749622Z","iopub.execute_input":"2024-12-30T10:23:49.750291Z","iopub.status.idle":"2024-12-30T10:23:50.069775Z","shell.execute_reply.started":"2024-12-30T10:23:49.750242Z","shell.execute_reply":"2024-12-30T10:23:50.068731Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[(df['Primary Tag'] == 'inappropriate') | (df['Primary Tag'] == 'child directed') | (df['Primary Tag'] == 'irrelevant')] \ndf['Primary Tag'].value_counts() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:23:50.071109Z","iopub.execute_input":"2024-12-30T10:23:50.071458Z","iopub.status.idle":"2024-12-30T10:23:50.089727Z","shell.execute_reply.started":"2024-12-30T10:23:50.071398Z","shell.execute_reply":"2024-12-30T10:23:50.088587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extracting video ids and primary labels\n\nvideo_ids = list(df['Video link'])\nprimary_labels = list(df['Primary Tag'])\nall_transcriptions = list(df['Transcription'])","metadata":{"id":"oR7rNEX3rl2I","execution":{"iopub.status.busy":"2024-12-30T10:23:50.091362Z","iopub.execute_input":"2024-12-30T10:23:50.091826Z","iopub.status.idle":"2024-12-30T10:23:50.098955Z","shell.execute_reply.started":"2024-12-30T10:23:50.091766Z","shell.execute_reply":"2024-12-30T10:23:50.097901Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(video_ids)","metadata":{"id":"J3gA6uDHSKoG","outputId":"7cf01806-916f-48f9-e17e-9e267e208d36","execution":{"iopub.status.busy":"2024-12-30T10:23:50.103926Z","iopub.execute_input":"2024-12-30T10:23:50.104435Z","iopub.status.idle":"2024-12-30T10:23:50.113308Z","shell.execute_reply.started":"2024-12-30T10:23:50.104365Z","shell.execute_reply":"2024-12-30T10:23:50.112157Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extracting data from transcripts\n\ntranscriptions = []\nlengths = []\n\nfor (i, id_) in enumerate(video_ids):\n    transcriptions.append(all_transcriptions[i].split(\"chunks\")[0])\n    lengths.append(len(all_transcriptions[i].split(\"chunks\")[0]))","metadata":{"id":"_7p-efT9sNMf","execution":{"iopub.status.busy":"2024-12-30T10:23:50.114942Z","iopub.execute_input":"2024-12-30T10:23:50.115380Z","iopub.status.idle":"2024-12-30T10:23:50.143862Z","shell.execute_reply.started":"2024-12-30T10:23:50.115334Z","shell.execute_reply":"2024-12-30T10:23:50.142751Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os \n\navailable_ids = os.listdir('/kaggle/input/youtube-data/Ads/Ads') \nlen(available_ids) ","metadata":{"execution":{"iopub.status.busy":"2024-12-30T10:23:50.145257Z","iopub.execute_input":"2024-12-30T10:23:50.145703Z","iopub.status.idle":"2024-12-30T10:23:50.169156Z","shell.execute_reply.started":"2024-12-30T10:23:50.145666Z","shell.execute_reply":"2024-12-30T10:23:50.168012Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"available_ad_ids = [] \n\nfor i in range(len(video_ids)): \n    if video_ids[i] in available_ids: \n        #if video_ids[i] not in ids_covered: \n        available_ad_ids.append(video_ids[i]) \n    else: \n        print(i, video_ids[i]) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:23:50.178373Z","iopub.execute_input":"2024-12-30T10:23:50.178853Z","iopub.status.idle":"2024-12-30T10:23:50.253207Z","shell.execute_reply.started":"2024-12-30T10:23:50.178797Z","shell.execute_reply":"2024-12-30T10:23:50.252084Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Mapping Ad Durations to Number of Images / Frames** ","metadata":{}},{"cell_type":"code","source":"durations_in_seconds = [] \nnum_frames = [] \n\nfor i in range(len(video_ids)): \n\n    contents_of_ad = os.listdir('/kaggle/input/youtube-data/Ads/Ads/' + video_ids[i]) \n    contents_of_ad.remove('audio.mp3') \n    video_path = '/kaggle/input/youtube-data/Ads/Ads/' + video_ids[i] + '/' + contents_of_ad[0] \n\n    cap = cv2.VideoCapture(video_path) \n\n    # Get the frames per second (fps) of the video\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    # Get the total number of frames in the video\n    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    # Calculate the duration in seconds\n    duration = total_frames / fps \n    \n    durations_in_seconds.append(round(duration, 2)) \n    num_frames.append(total_frames) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:23:50.254577Z","iopub.execute_input":"2024-12-30T10:23:50.254957Z","iopub.status.idle":"2024-12-30T10:24:19.632303Z","shell.execute_reply.started":"2024-12-30T10:23:50.254916Z","shell.execute_reply":"2024-12-30T10:24:19.631295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculating number of images needed for each ad \n\n# < 1 minute: 1 image \n# 1 - 2 minutes: 2 images \n# 2 - 3 minutes: 3 images \n# 3 - 4 minutes: 4 images \n# > 4 minutes: 5 images \n\nnum_images = [int(((x // 60) + 1)) if ((x // 60) + 1) <= 5 else 5 for x in durations_in_seconds] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:24:19.633672Z","iopub.execute_input":"2024-12-30T10:24:19.634085Z","iopub.status.idle":"2024-12-30T10:24:19.640228Z","shell.execute_reply.started":"2024-12-30T10:24:19.634042Z","shell.execute_reply":"2024-12-30T10:24:19.639074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\n\n# Count the frequency of each number\nfrequency = Counter(num_images)\nvalues = list(frequency.keys())\ncounts = list(frequency.values())\n\nplt.bar(values, counts)\n\nplt.xlabel(\"Number of Grids for Ad\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Number of grids for each ad\")\n\n# Show the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:24:19.641715Z","iopub.execute_input":"2024-12-30T10:24:19.642042Z","iopub.status.idle":"2024-12-30T10:24:19.925945Z","shell.execute_reply.started":"2024-12-30T10:24:19.642012Z","shell.execute_reply":"2024-12-30T10:24:19.924855Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Setting up Model** ","metadata":{"id":"MyTbSTVcehXl"}},{"cell_type":"code","source":"os.environ[\"ANTHROPIC_API_KEY\"] = \"\" # add key here ","metadata":{"id":"d4BaWth9kkKa","execution":{"iopub.status.busy":"2024-12-30T10:24:19.927521Z","iopub.execute_input":"2024-12-30T10:24:19.927878Z","iopub.status.idle":"2024-12-30T10:24:19.933069Z","shell.execute_reply.started":"2024-12-30T10:24:19.927846Z","shell.execute_reply":"2024-12-30T10:24:19.931781Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"client = anthropic.Anthropic() ","metadata":{"id":"WgJcICGYd2Q8","execution":{"iopub.status.busy":"2024-12-30T10:24:19.934593Z","iopub.execute_input":"2024-12-30T10:24:19.934943Z","iopub.status.idle":"2024-12-30T10:24:19.958456Z","shell.execute_reply.started":"2024-12-30T10:24:19.934911Z","shell.execute_reply":"2024-12-30T10:24:19.957321Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Prompt** ","metadata":{"id":"V2kv113nelLF"}},{"cell_type":"code","source":"prompt =  \"\"\" A video is inappropriate for children if it contains physical violence (cartoonish, realistic, inclusion of gore),\n  interpersonal violence (such as bullying, pranks, meanness, humor at expense of others, belittling ,controlling, talking down to, or manipulating others),\n  self-harm or suicide,\n  elements of scariness (horror, scary themes, creepy or suspenseful themes, loud and violent content, zombies, skeletons, scary masks, scary clowns, blood, fire in dangerous settings,\n  mentally distressing scenes (car crashes, medical procedures etc),\n  extreme stunts (depictions of life endangering or high-risk physical activities/challenges),\n  sexual content (innuendos, overt sexual behavior, nudity, sexual themes, sexual suggestiveness),\n  discrimination (based on race/ethnicity/gender/ability),\n  offensive language (from mild words such as \"idiot\" and \"stupid\" to severe swearing, hate speech or signs),\n  drinking, drugs, smoking, gambling, crude humour,\n  inappropriate clothing (with significant skin exposure),\n  death,\n  intimacy (sensual physical contact, marriage, romance, dating and relationships, apparent romantic conduct between subjects/actors).\n  romantic songs and music.\n\n  You have been given image grid(s) (3 x 2) representing a video and it's audio transcription.\n  Consider that you are a Parent. Based on both the visual content and audio transcription, is the video ad provided inappropriate for children under 9 based on this definition?\n  If it is, label it as \"inappropriate\".\n  If it is not, label it as \"appropriate\".\n\n  Justify your conclusion.\n\n  Also, mention what language is being spoken if there is any.\n\n  Make the outputs in JSON format (keys inlcude 'label' (only one of: 'inappropriate', 'child-directed', or 'irrelevant'), 'justification', and 'language' (if any is spoken)).\n  Please say nothing else outside of this json format. \n\"\"\"","metadata":{"id":"3UX0f8bHemQW","execution":{"iopub.status.busy":"2024-12-30T10:24:19.959894Z","iopub.execute_input":"2024-12-30T10:24:19.960229Z","iopub.status.idle":"2024-12-30T10:24:19.967045Z","shell.execute_reply.started":"2024-12-30T10:24:19.960196Z","shell.execute_reply":"2024-12-30T10:24:19.965802Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import base64\nimport httpx\n\n# Function to encode the image\n\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file: \n        image_media_type = \"image/png\" \n        image_data = base64.b64encode(image_file.read()).decode('utf-8') \n        return image_media_type, image_data \n","metadata":{"id":"ZLdyktC4lJDt","execution":{"iopub.status.busy":"2024-12-30T10:24:19.968536Z","iopub.execute_input":"2024-12-30T10:24:19.969626Z","iopub.status.idle":"2024-12-30T10:24:19.982111Z","shell.execute_reply.started":"2024-12-30T10:24:19.969588Z","shell.execute_reply":"2024-12-30T10:24:19.981106Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def classify_video_with_images(text_input, audio_transcription, image_paths):\n    \n    image_contents = [\n        {\n            \"type\": \"image\",\n            \"source\": {\n                \"type\": \"base64\",\n                \"media_type\": img_media_type,\n                \"data\": img_data,\n            }\n        }\n        for image_path in image_paths\n        for img_media_type, img_data in [encode_image(image_path)]\n    ]\n    \n    # Prepare the payload with images and text\n    response = client.messages.create(\n        model=\"claude-3-5-sonnet-20240620\",\n        system=\"You are a content classification assistant that evaluates image frames and audio transcriptions from videos according to strict guidelines.\",\n        max_tokens=1024,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    *image_contents,  # Unpack the list of images\n                    {\n                        \"type\": \"text\",\n                        \"text\": text_input\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": audio_transcription\n                    }\n                ],\n            }\n        ]\n    )\n\n    return response\n","metadata":{"execution":{"iopub.status.busy":"2024-12-30T10:24:19.998621Z","iopub.execute_input":"2024-12-30T10:24:19.998986Z","iopub.status.idle":"2024-12-30T10:24:20.008429Z","shell.execute_reply.started":"2024-12-30T10:24:19.998952Z","shell.execute_reply":"2024-12-30T10:24:20.007293Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Extracting Images from Videos** ","metadata":{"id":"w8esg7q8hSTW"}},{"cell_type":"code","source":"def detect_scenes(video_path, threshold = 30):\n    \"\"\"Detect scenes in a video and return scene start and end frames.\"\"\"\n    scene_list = []\n    while len(scene_list) < 6 and threshold > 0:\n        threshold //= 2\n    \n        video = open_video(video_path)\n        scene_manager = SceneManager()\n        scene_manager.add_detector(ContentDetector(threshold=threshold))\n    \n        scene_manager.detect_scenes(video)\n        scene_list = scene_manager.get_scene_list()\n    \n    return scene_list\n\n\ndef get_top_n_longest_scenes(scene_list, n):\n    '''Return the top n longest scenes with start and end frame indices.'''\n    scene_durations = [(start, end - start) for start, end in scene_list]\n    scene_durations.sort(key=lambda x: x[1], reverse=True)\n\n    # Top n longest scenes with start and end frame indices\n    longest_scenes = [(start, start + duration) for start, duration in scene_durations[:n]]\n    return longest_scenes\n\n\ndef sort_scenes_by_frame(scenes_list):\n    '''Sort scenes by their start frame number.'''\n    sorted_scenes = sorted(scenes_list, key=lambda scene: scene[0].get_frames())\n    return sorted_scenes\n\n\ndef get_num_grids(video_path):\n    '''Get number of grids to be created'''\n    cap = cv2.VideoCapture(video_path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    duration = total_frames / fps\n\n    # Calculate number of grids based on the duration\n    duration = round(duration, 2)\n    if ((duration // 60) + 1) <= 5:\n        return int(((duration // 60) + 1))\n    else:\n        return 5\n\n\ndef extract_k_frames_from_scene(video_path, scene, k):\n    '''Extract k frames evenly spaced from each scene.'''\n    # Extract frame numbers from scene start and end\n    start_frame = scene[0].get_frames() + 1\n    end_frame = scene[1].get_frames() - 1\n\n    # Create k equally spaced frame indices within the scene's range\n    frame_indices = np.linspace(start_frame, end_frame, k, dtype=int)\n    \n    cap = cv2.VideoCapture(video_path)\n    frames = []\n\n    # Extract frames from calculated indices\n    for frame_no in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n        ret, frame = cap.read()\n        if ret:\n            frames.append(frame)\n    \n    cap.release()\n    return frames\n\n\ndef create_image_grid(frames, grid_size=(1000, 1000)):\n    '''Arrange 6 frames into a 3x2 grid and resize to the specified grid size.'''\n    # Ensure all frames have the same size for concatenation\n    frames = [cv2.resize(frame, (640, 360)) for frame in frames]  # Resize to a common size like 640x360\n    rows = [np.concatenate(frames[i:i+2], axis=1) for i in range(0, 6, 2)]\n    image_grid = np.concatenate(rows, axis=0)\n    \n    return np.array(Image.fromarray(image_grid).resize(grid_size))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:24:20.009747Z","iopub.execute_input":"2024-12-30T10:24:20.010239Z","iopub.status.idle":"2024-12-30T10:24:20.030084Z","shell.execute_reply.started":"2024-12-30T10:24:20.010203Z","shell.execute_reply":"2024-12-30T10:24:20.028996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_images(video_path, n=6):\n    ''' 1. Detect scenes\n        2. Get k; where k = num_grids\n        3. Get the 6 longest scenes\n        4. Sort scenes wrt frame numbers\n        5. Extract n * k frames\n        6. Create k image grids of n frames each\n     '''\n    scene_list = detect_scenes(video_path)\n    k = get_num_grids(video_path)\n    #k = 1 # For Single Grid of Major Scene Frames\n    longest_scenes = get_top_n_longest_scenes(scene_list, n*k)\n    scenes = sort_scenes_by_frame(longest_scenes)\n\n    frames = []\n    for scene in scenes:\n        frames.extend(extract_k_frames_from_scene(video_path, scene, 1))\n\n    grids = []\n    for i in range(k):\n        start_idx = i * n\n        end_idx = start_idx + n\n        grid_frames = frames[start_idx:end_idx]\n        grid = create_image_grid(grid_frames, grid_size=(1000, 1000))\n        grids.append(grid)\n\n    return grids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:24:20.033758Z","iopub.execute_input":"2024-12-30T10:24:20.034163Z","iopub.status.idle":"2024-12-30T10:24:20.046682Z","shell.execute_reply.started":"2024-12-30T10:24:20.034128Z","shell.execute_reply":"2024-12-30T10:24:20.045590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ids = []\nlabels = []\nresponses = []\npredicted_labels = [] \nremaining = [] \n\nimg_dir = '/kaggle/working/Images'\nif not os.path.exists(img_dir):\n    os.makedirs(img_dir)\n\nfor i in range(len(video_ids)): \n    \n    if video_ids[i] in available_ad_ids:\n\n        contents_of_ad = os.listdir('/kaggle/input/youtube-data/Ads/Ads/' + video_ids[i]) \n        contents_of_ad.remove('audio.mp3') \n        video_path = '/kaggle/input/youtube-data/Ads/Ads/' + video_ids[i] + '/' + contents_of_ad[0] \n\n        try: \n            print(i, video_path)\n\n            # Extract multiple images representative of the video\n            images = get_images(video_path) \n\n            # Save each image returned by extract_images_of_frames\n            image_paths = []\n            for idx, img in enumerate(images):\n                # Convert NumPy array to PIL image\n                image = Image.fromarray(img)\n                \n                # Save the image to a file \n                image_name = f\"{video_ids[i]}_{idx + 1}.png\"\n                image_path = os.path.join(img_dir, image_name)\n                image.save(image_path)\n                image_paths.append(image_path)\n                print(image_path)\n                \n            # Display the images\n            fig, axes = plt.subplots(1, len(images), figsize=(10, 3))\n            if len(images) == 1:\n                axes.imshow(images[0])\n                axes.axis('off')\n            else:\n                for ax, img in zip(axes, images):\n                    ax.imshow(img)\n                    ax.axis('off')\n\n            plt.tight_layout()\n            plt.show()\n\n            audio_transcription = transcriptions[i]\n\n            # Make inference\n            classification_response = classify_video_with_images(prompt, audio_transcription, image_paths)\n            temp_id = video_ids[i] \n            temp_label = primary_labels[i] \n            temp_response = classification_response.content[0].text \n            pred_temp = json.loads(classification_response.content[0].text).get('label') \n\n            print('Id:', temp_id, '. Primary Label:', temp_label, '\\nResponse:', temp_response) \n\n            ids.append(temp_id) \n            labels.append(temp_label) \n            responses.append(temp_response) \n            predicted_labels.append(pred_temp) \n            \n        except: \n            print('failed for ', i) \n            remaining.append(video_ids[i])\n\n        time.sleep(20)\n","metadata":{"id":"fAc-89lDDknS","outputId":"df6ab726-c98f-4d74-b330-5dcbd5eadf36","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:33:11.641037Z","iopub.execute_input":"2024-12-30T10:33:11.641515Z","iopub.status.idle":"2024-12-30T10:33:38.518040Z","shell.execute_reply.started":"2024-12-30T10:33:11.641476Z","shell.execute_reply":"2024-12-30T10:33:38.516618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(responses)): \n    print('True Label: ', labels[i], '\\tPrediction: ', predicted_labels[i]) ","metadata":{"id":"aqDlWLZcZJHL","execution":{"iopub.status.busy":"2024-12-30T10:33:38.578287Z","iopub.execute_input":"2024-12-30T10:33:38.578903Z","iopub.status.idle":"2024-12-30T10:33:38.591100Z","shell.execute_reply.started":"2024-12-30T10:33:38.578842Z","shell.execute_reply":"2024-12-30T10:33:38.589587Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df = pd.DataFrame({\n    'Video Id': ids,\n    'Primary Label': labels,\n    'Predicted Label': predicted_labels,\n    'Response': responses\n})\n\nnew_df.head() ","metadata":{"id":"6JsiW7gbvxRA","execution":{"iopub.status.busy":"2024-12-30T10:33:38.593079Z","iopub.execute_input":"2024-12-30T10:33:38.593641Z","iopub.status.idle":"2024-12-30T10:33:38.618448Z","shell.execute_reply.started":"2024-12-30T10:33:38.593582Z","shell.execute_reply":"2024-12-30T10:33:38.617180Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_dir = '/kaggle/working/results'\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:33:38.619845Z","iopub.execute_input":"2024-12-30T10:33:38.620324Z","iopub.status.idle":"2024-12-30T10:33:38.635546Z","shell.execute_reply.started":"2024-12-30T10:33:38.620277Z","shell.execute_reply":"2024-12-30T10:33:38.633324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df.to_csv('/kaggle/working/results/DAVSP_claude-3.5', index=False) ","metadata":{"id":"Tv1_5sfJwRfu","execution":{"iopub.status.busy":"2024-12-30T10:33:38.637767Z","iopub.execute_input":"2024-12-30T10:33:38.638233Z","iopub.status.idle":"2024-12-30T10:33:38.648978Z","shell.execute_reply.started":"2024-12-30T10:33:38.638177Z","shell.execute_reply":"2024-12-30T10:33:38.647703Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Changing to binary lists \n\npredictions = [1 if pred == 'inappropriate' else 0 for pred in predicted_labels] \nground_truths = [1 if label == 'inappropriate' else 0 for label in labels] ","metadata":{"id":"4hsLj8P8Mg-e","execution":{"iopub.status.busy":"2024-12-30T10:33:38.651962Z","iopub.execute_input":"2024-12-30T10:33:38.652541Z","iopub.status.idle":"2024-12-30T10:33:38.659284Z","shell.execute_reply.started":"2024-12-30T10:33:38.652495Z","shell.execute_reply":"2024-12-30T10:33:38.658031Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Obtaining classification report \nfrom sklearn.metrics import classification_report \n\nreport = classification_report(ground_truths, predictions) \nprint(report) ","metadata":{"execution":{"iopub.status.busy":"2024-12-30T10:33:38.661058Z","iopub.execute_input":"2024-12-30T10:33:38.661855Z","iopub.status.idle":"2024-12-30T10:33:38.682618Z","shell.execute_reply.started":"2024-12-30T10:33:38.661792Z","shell.execute_reply":"2024-12-30T10:33:38.681251Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(ground_truths, predictions)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Child Directed', 'Inapproriate'], yticklabels=['Child Directed', 'Inapproriate'])\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:33:38.684322Z","iopub.execute_input":"2024-12-30T10:33:38.684858Z","iopub.status.idle":"2024-12-30T10:33:38.957462Z","shell.execute_reply.started":"2024-12-30T10:33:38.684807Z","shell.execute_reply":"2024-12-30T10:33:38.956017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}