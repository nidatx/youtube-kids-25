{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10328861,"sourceType":"datasetVersion","datasetId":6395471},{"sourceId":10349036,"sourceType":"datasetVersion","datasetId":6296913},{"sourceId":10358109,"sourceType":"datasetVersion","datasetId":5987034},{"sourceId":10549716,"sourceType":"datasetVersion","datasetId":6179624},{"sourceId":10550838,"sourceType":"datasetVersion","datasetId":6411015}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"id":"exmY-abTefFW"}},{"cell_type":"code","source":"!pip install openai\n!pip install ffmpeg-python\n!pip install av\n!pip install scenedetect","metadata":{"id":"Bi3c0wLhh-Rf","outputId":"44c463e5-7c4c-45cf-daca-75620249739c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport time\nimport json\nimport torch\nimport random\nimport ffmpeg\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom glob import glob\nimport soundfile as sf\nfrom openai import OpenAI\nfrom json import loads,dumps\nimport matplotlib.pyplot as plt\n#from pydub import AudioSegment\nfrom scipy.signal import resample\nimport typing_extensions as typing\nfrom google.generativeai.types import HarmCategory, HarmBlockThreshold\nfrom scenedetect import open_video, VideoStreamCv2, SceneManager\nfrom scenedetect.detectors import ContentDetector","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"limit = 5000 ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"# Setting up video directories \n\ntrue1dir = '/kaggle/input/video-classification-data/Videos/Videos/videos-true' \ntrue2dir = '/kaggle/input/video-classification-data/Videos/Videos/videos-true-2' \nkids_dir = '/kaggle/input/video-classification-data/Made-For-Kids/Made-For-Kids'\n\nfalse1dir = '/kaggle/input/video-classification-data/Videos/Videos/videos-false' \nfalse2dir = '/kaggle/input/video-classification-data/Videos/Videos/videos-false-2' \nnon_kids_dir = '/kaggle/input/video-classification-data/Non Made For Kids/Non Made For Kids'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Functions to extract corresponding transcriptions and ground truths \n\ndef get_transcriptions_and_paths(vids_dir, ground_truths, transcriptions_path): \n\n    ids = os.listdir(vids_dir) \n\n    labels_df = pd.DataFrame({ \n        'IDs': ids, \n        'Labels': [ground_truths] * len(ids) \n    }) \n    \n    transcriptions_df = pd.read_csv(transcriptions_path) \n    \n    # Merging dataframes \n    df = pd.merge(labels_df, transcriptions_df, left_on='IDs', right_on='Video Id') \n\n    # Extracting transcriptions \n    ids = list(df['IDs']) \n    paths = [vids_dir + '/' + id for id in ids] \n    all_transcriptions = list(df['Transcription']) \n\n    # Extracting data from transcripts \n    transcriptions_temp = [] \n    lengths = [] \n    for (i, id_) in enumerate(ids): \n        transcriptions_temp.append(all_transcriptions[i].split(\"chunks\")[0]) \n        lengths.append(len(all_transcriptions[i].split(\"chunks\")[0])) \n\n    # setting limit on transcription length \n    transcriptions = [x[:limit] + '...' if len(x) > limit else x for x in transcriptions_temp] \n\n    return paths, transcriptions, [ground_truths] * len(ids) ","metadata":{"id":"1ksUpR0ibKve","outputId":"6880ee21-f703-4761-dfcd-842e1f8b61d9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading paths, transcriptions, and labels \n\ntrue1_paths, true1_transcriptions, true1_labels = get_transcriptions_and_paths(true1dir, 'Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/true-1-translated-transcriptions.csv') \ntrue2_paths, true2_transcriptions, true2_labels = get_transcriptions_and_paths(true2dir, 'Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/true-2-translated-transcriptions.csv') \nkids_paths, kids_transcriptions, kids_labels = get_transcriptions_and_paths(kids_dir, 'Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/made-for-kids_translated_transcriptions.csv') \n\nfalse1_paths, false1_transcriptions, false1_labels = get_transcriptions_and_paths(false1dir, 'Not Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/false-1-translated-transcriptions.csv') \nfalse2_paths, false2_transcriptions, false2_labels = get_transcriptions_and_paths(false2dir, 'Not Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/false-2-translated-transcriptions.csv') \nnon_kids_paths, non_kids_transcriptions, non_kids_labels = get_transcriptions_and_paths(non_kids_dir, 'Not Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/non-made-for-kids_translated_transcriptions.csv') ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final combined list of paths and ground labels \n\npaths = [] \ntranscriptions = [] \nprimary_labels = [] \n\npaths.extend(false1_paths) \npaths.extend(true1_paths) \npaths.extend(false2_paths) \npaths.extend(true2_paths) \npaths.extend(non_kids_paths)\npaths.extend(kids_paths) \n\nprimary_labels.extend(false1_labels) \nprimary_labels.extend(true1_labels) \nprimary_labels.extend(false2_labels) \nprimary_labels.extend(true2_labels) \nprimary_labels.extend(non_kids_labels) \nprimary_labels.extend(kids_labels) \n\ntranscriptions.extend(false1_transcriptions) \ntranscriptions.extend(true1_transcriptions) \ntranscriptions.extend(false2_transcriptions) \ntranscriptions.extend(true2_transcriptions) \ntranscriptions.extend(non_kids_transcriptions) \ntranscriptions.extend(kids_transcriptions) \n\nvideo_ids = [path.split('/')[-1] for path in paths]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(paths), len(primary_labels), len(transcriptions), len(video_ids)) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Extracting Images** ","metadata":{}},{"cell_type":"code","source":"def detect_scenes(video_path, threshold = 30):\n    \"\"\"Detect scenes in a video and return scene start and end frames.\"\"\"\n    scene_list = []\n    while len(scene_list) < 6 and threshold > 0:\n        threshold //= 2\n    \n        video = open_video(video_path)\n        scene_manager = SceneManager()\n        scene_manager.add_detector(ContentDetector(threshold=threshold))\n    \n        scene_manager.detect_scenes(video)\n        scene_list = scene_manager.get_scene_list()\n    \n    return scene_list\n\n\ndef get_top_n_longest_scenes(scene_list, n):\n    '''Return the top n longest scenes with start and end frame indices.'''\n    scene_durations = [(start, end - start) for start, end in scene_list]\n    scene_durations.sort(key=lambda x: x[1], reverse=True)\n\n    # Top n longest scenes with start and end frame indices\n    longest_scenes = [(start, start + duration) for start, duration in scene_durations[:n]]\n    return longest_scenes\n\ndef sort_scenes_by_frame(scenes_list):\n    '''Sort scenes by their start frame number.'''\n    sorted_scenes = sorted(scenes_list, key=lambda scene: scene[0].get_frames())\n    return sorted_scenes\n\n\ndef get_num_grids(video_path):\n    '''Get number of grids to be created'''\n    cap = cv2.VideoCapture(video_path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    duration = total_frames / fps\n\n    # Calculate number of grids based on the duration\n    duration = round(duration, 2)\n    if ((duration // 60) + 1) <= 5:\n        return int(((duration // 60) + 1))\n    else:\n        return 5\n\ndef extract_k_frames_from_scene(video_path, scene, k):\n    '''Extract k frames evenly spaced from each scene.'''\n    # Extract frame numbers from scene start and end\n    start_frame = scene[0].get_frames() + 1\n    end_frame = scene[1].get_frames() - 1\n\n    # Create k equally spaced frame indices within the scene's range\n    frame_indices = np.linspace(start_frame, end_frame, k, dtype=int)\n    \n    cap = cv2.VideoCapture(video_path)\n    frames = []\n\n    # Extract frames from calculated indices\n    for frame_no in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n        ret, frame = cap.read()\n        if ret:\n            frames.append(frame)\n    \n    cap.release()\n    return frames\n\n\ndef create_image_grid(frames, grid_size=(1000, 1000)):\n    '''Arrange 6 frames into a 3x2 grid and resize to the specified grid size.'''\n    # Ensure all frames have the same size for concatenation\n    frames = [cv2.resize(frame, (640, 360)) for frame in frames]  # Resize to a common size like 640x360\n    rows = [np.concatenate(frames[i:i+2], axis=1) for i in range(0, 6, 2)]\n    image_grid = np.concatenate(rows, axis=0)\n    \n    return np.array(Image.fromarray(image_grid).resize(grid_size))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_images(video_path, n=6):\n    ''' 1. Detect scenes\n        2. Get k; where k = num_grids\n        3. Get the 6 longest scenes\n        4. Sort scenes wrt frame numbers\n        5. Extract n * k frames\n        6. Create k image grids of n frames each\n     '''\n    scene_list = detect_scenes(video_path)\n    k = get_num_grids(video_path)\n    #k = 1 # For Single Grid of Major Scene Frames\n    longest_scenes = get_top_n_longest_scenes(scene_list, n*k)\n    scenes = sort_scenes_by_frame(longest_scenes)\n\n    frames = []\n    for scene in scenes:\n        frames.extend(extract_k_frames_from_scene(video_path, scene, 1))\n\n    grids = []\n    for i in range(k):\n        start_idx = i * n\n        end_idx = start_idx + n\n        grid_frames = frames[start_idx:end_idx]\n        grid = create_image_grid(grid_frames, grid_size=(1000, 1000))\n        grids.append(grid)\n\n    return grids","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_images_2(video_path, n = 6):\n    ''' \n    Caters the videos where <= 6 scenes are extracted\n    '''\n\n    # Step 1: Detect scenes\n    scene_list = detect_scenes(video_path)\n    if not scene_list:\n        return []  # Handle case where no scenes are detected\n    \n    # Step 2: Get the number of grids (k)\n    k = get_num_grids(video_path)\n    \n    # Total number of frames needed\n    total_frames_needed = n * k\n    available_scenes = len(scene_list)\n\n    if available_scenes == 0:\n        return []  # Handle case where no scenes are available for frame extraction\n    \n    # Step 3: Adjust the number of frames extracted per scene\n    frames_per_scene = total_frames_needed // available_scenes\n    remaining_frames = total_frames_needed % available_scenes\n\n    if available_scenes == 1:\n        frames_per_scene = total_frames_needed  # Assign all frames to the single scene\n        remaining_frames = 0  # No remaining frames\n\n    # Extract frames evenly across all scenes\n    frames = []\n    for i, scene in enumerate(scene_list):\n        num_frames = frames_per_scene + (1 if i < remaining_frames else 0)\n        frames.extend(extract_k_frames_from_scene(video_path, scene, num_frames))\n\n    # Ensure we have exactly total_frames_needed frames\n    frames = frames[:total_frames_needed]\n    \n    # Step 4: Create image grids\n    grids = []\n    if len(frames) < n:  # If fewer frames are available than needed\n        # Repeat the available frames until there are enough\n        frames = frames * (n // len(frames)) + frames[:(n % len(frames))]\n\n    # Create the grids\n    for i in range(k):\n        start_idx = i * n\n        end_idx = start_idx + n\n        grid_frames = frames[start_idx:end_idx]\n        if len(grid_frames) > 0:  # Ensure we have frames to create the grid\n            grid = create_image_grid(grid_frames, grid_size=(1000, 1000))\n            grids.append(grid)\n    \n    return grids","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Defining Model** ","metadata":{}},{"cell_type":"code","source":"apikey = \"\" # add key here ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"client = OpenAI(api_key=apikey) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_schema = {\n  \"type\": \"object\",\n  \"properties\": {\n    \"label\": {\n      \"type\": \"string\",\n      \"enum\": [\"Child Directed\", \"Not Child Directed\"]\n    },\n    \"justification\": {\n      \"type\": \"string\",\n      \"minLength\": 10\n    }, \n    \"languages\": {\n        \"type\": \"array\", \n        \"items\": {\n            \"type\": \"string\" \n        }\n        \n    }\n  },\n  \"required\": [\"label\", \"justification\", \"languages\"]\n} ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import base64\nimport requests\n\ndef encode_image(image_path):\n    \"\"\"\n    Encodes an image at the given path to a base64 string.\n    \"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\ndef classify_video_with_images(text_input, audio_transcription, image_paths):\n    \"\"\"\n    Sends text, audio transcription, and multiple images to the API for classification.\n    \n    Parameters:\n    - text_input: str, the input text.\n    - audio_transcription: str, transcription of the audio.\n    - image_paths: list of str, paths to the images.\n\n    Returns:\n    - response: The API response.\n    \"\"\"\n    # Encode all images to base64\n    encoded_images = [\n        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(path)}\"}}\n        for path in image_paths\n    ]\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {apikey}\"\n    }\n\n    payload = {\n        \"model\": \"gpt-4o-2024-08-06\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": text_input},\n                    {\"type\": \"text\", \"text\": 'Audio transcription: ' + audio_transcription},\n                ] + encoded_images  # Append all encoded images to the message content\n            }\n        ],\n        \"response_format\": {\n            \"type\": \"json_schema\",\n            \"json_schema\": {\n                \"name\": \"output_schema\",\n                \"schema\": output_schema\n            }\n        },\n        \"max_tokens\": 300\n    }\n\n    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n\n    return response","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Prompt** ","metadata":{}},{"cell_type":"code","source":"# Create the prompt.\nprompt = \"\"\"\n    A piece of content is child-directed if it meets any of these criteria:\n\n    Designed for Children:\n    The content is explicitly created with children as the intended audience, such as:\n      - Educational videos for kids.\n      - Child-friendly video games with cartoonish graphics and non-violent gameplay and story-telling\n      - Animated movies/clips suitable for family viewing\n      - Simple crafts, activities, or demonstrations aimed at children.\n\n    Child-Appealing Elements:\n    The content includes features commonly enjoyed by children, such as:\n    - Colorful, cartoonish, or animated visuals (e.g., animals, anthropomorphic characters, fantasy creatures).\n    - Light-hearted portrayals of themes like mischief or conflict, but without mature visuals, strong language, or adult humor.\n    - Whimsical, or playful themes.\n    - Non-violent and simplified gameplay or narratives.\n    - Content with rhythmic elements (e.g., chants, exclamations, or songs appealing to young viewers).\n\n    Important Considerations:\n    - Content can be child-directed even if it also appeals to older audiences.\n    - Gaming content is child-directed if it features family-friendly gameplay and cartoonish visuals.\n    - Movie/TV clips are child-directed if rated G/PG.\n    - Presence of mild peril or conflict is acceptable if presented appropriately.\n\n    Exclusions:\n    - Do not label content as \"Child Directed\" if it features significant:\n      - Dark, mature, or violent themes.\n      - Romantic subplots or adult humor.\n      - Complex dialogue or advanced vocabulary inappropriate for children.\n      - Songs that are not nursery rhymes or explicitly created for children.\n    - Recipe, DIY, or instructional videos unless simplified for children (e.g., \"Cooking for Kids\").\n\n    Instructions:\n    Labeling:\n    If the video meets the child-directed criteria label it as \"Child Directed\".\n    If it does not meet these criteria label it as \"Not Child Directed\".\n\n    Indicate the spoken language if any.\n    Provide a brief justification explaining why the video is considered child-directed or not.\n\n    Format the output in JSON.\n  \"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Running Model on Dataset** ","metadata":{"id":"w8esg7q8hSTW"}},{"cell_type":"code","source":"ids = []\nground_truths = []\nresponses = []\npredicted_labels = [] \nlanguages = [] \nremaining = [] \n\nimg_dir = '/kaggle/working/Images'\nif not os.path.exists(img_dir):\n    os.makedirs(img_dir)\n\nfor i in range(len(paths)):\n\n    try: \n       \n        contents_of_ad = os.listdir(paths[i]) \n        contents_of_ad.remove('audio.mp3') \n        video_path = paths[i] + '/' + contents_of_ad[0] \n        audio_path = paths[i] + '/audio.mp3' \n    \n        print('\\n', i, video_path)\n        \n        # Extract multiple images representative of the video\n        try:\n            images = get_images(video_path) \n        except Exception as e:\n            print('\\ncalling get_images_2() ', i, 'Error:', str(e))\n            images = get_images_2(video_path)\n\n        # Save each image returned by extract_images_of_frames\n        image_paths = []\n        for idx, img in enumerate(images):\n            # Convert NumPy array to PIL image\n            image = Image.fromarray(img)\n            \n            # Save the image to a file\n            image_name = f\"{video_ids[i]}_{idx + 1}.png\"\n            image_path = os.path.join(img_dir, image_name)\n            image.save(image_path)\n            image_paths.append(image_path)\n            \n        # Display the images\n        fig, axes = plt.subplots(1, len(images), figsize=(10, 3))\n        if len(images) == 1:\n            axes.imshow(images[0])\n            axes.axis('off')\n        else:\n            for ax, img in zip(axes, images):\n                ax.imshow(img)\n                ax.axis('off')\n\n        plt.tight_layout()\n        plt.show()\n\n        audio_transcription = transcriptions[i] \n\n        \n        classification_response = classify_video_with_images(prompt, audio_transcription, image_paths)\n    \n        temp_id = video_ids[i] \n        temp_label = primary_labels[i] \n        temp_predicted_label = json.loads(classification_response.json()['choices'][0]['message']['content']).get('label') \n        temp_response = classification_response.json()['choices'][0]['message']['content'] \n        temp_languages = json.loads(classification_response.json()['choices'][0]['message']['content']).get('languages') \n\n        ids.append(temp_id)\n        ground_truths.append(temp_label)\n        predicted_labels.append(temp_predicted_label)\n        responses.append(temp_response)\n        languages.append(temp_languages) \n\n        print('\\nPrimary Label:', primary_labels[i], '. Response:', classification_response.json()['choices'][0]['message']['content'])\n\n\n    except Exception as e:\n        print('\\nImage extraction failed for ', i, 'Error:', str(e))\n        remaining.append(video_ids[i])\n    \n    time.sleep(20) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(ids)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# At the end, print remaining videos \n\nprint(\"Remaining videos with errors:\", remaining) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(responses)): \n    print('True Label: ', ground_truths[i], '\\tPrediction: ', predicted_labels[i]) ","metadata":{"id":"aqDlWLZcZJHL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df = pd.DataFrame({\n    'Video Id': ids,\n    'Primary Label': ground_truths,\n    'Predicted Label': predicted_labels,\n    'Response': responses, \n    'Languages': languages \n})\n\nnew_df.head() ","metadata":{"id":"6JsiW7gbvxRA","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_dir = '/kaggle/working/results'\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df.to_csv('/kaggle/working/results/DAVSP_GPT-4o.csv', index=False)","metadata":{"id":"Tv1_5sfJwRfu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Changing to binary lists \n\npredictions = [1 if pred == 'Not Child Directed' else 0 for pred in predicted_labels] \nground_truths = [1 if label == 'Not Child Directed' else 0 for label in ground_truths] ","metadata":{"id":"4hsLj8P8Mg-e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Obtaining classification report \nfrom sklearn.metrics import classification_report \n\nreport = classification_report(ground_truths, predictions) \nprint(report) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(ground_truths, predictions)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Child Directed', 'Child Directed'], yticklabels=['Not Child Directed', 'Child Directed'])\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}