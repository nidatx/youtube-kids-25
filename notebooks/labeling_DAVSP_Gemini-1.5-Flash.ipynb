{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10328861,"sourceType":"datasetVersion","datasetId":6395471},{"sourceId":10349036,"sourceType":"datasetVersion","datasetId":6296913},{"sourceId":10358109,"sourceType":"datasetVersion","datasetId":5987034},{"sourceId":10549716,"sourceType":"datasetVersion","datasetId":6179624},{"sourceId":10550838,"sourceType":"datasetVersion","datasetId":6411015}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"id":"exmY-abTefFW"}},{"cell_type":"code","source":"!pip install ffmpeg-python\n!pip install av\n!pip install -q -U google-generativeai\n!pip install --upgrade pip\n!pip install --upgrade transformers datasets[audio] accelerate\n!pip install scenedetect","metadata":{"id":"Bi3c0wLhh-Rf","outputId":"44c463e5-7c4c-45cf-daca-75620249739c","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:06:02.464032Z","iopub.execute_input":"2024-12-30T06:06:02.464987Z","iopub.status.idle":"2024-12-30T06:06:29.359949Z","shell.execute_reply.started":"2024-12-30T06:06:02.464949Z","shell.execute_reply":"2024-12-30T06:06:29.358791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport time\nimport torch\nimport random\nimport ffmpeg\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom glob import glob\nimport soundfile as sf\nfrom json import loads,dumps\n#from pydub import AudioSegment\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom scipy.signal import resample\nimport typing_extensions as typing\nfrom google.generativeai.types import HarmCategory, HarmBlockThreshold\nfrom scenedetect import open_video, VideoStreamCv2, SceneManager\nfrom scenedetect.detectors import ContentDetector","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:06:34.103172Z","iopub.execute_input":"2024-12-30T06:06:34.103586Z","iopub.status.idle":"2024-12-30T06:06:34.110257Z","shell.execute_reply.started":"2024-12-30T06:06:34.103549Z","shell.execute_reply":"2024-12-30T06:06:34.109155Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"# Setting up video directories \n\ntrue1dir = '/kaggle/input/video-classification-data/Videos/Videos/videos-true' \ntrue2dir = '/kaggle/input/video-classification-data/Videos/Videos/videos-true-2' \nkids_dir = '/kaggle/input/video-classification-data/made-for-kids/made-for-kids'\n\nfalse1dir = '/kaggle/input/video-classification-data/Videos/Videos/videos-false' \nfalse2dir = '/kaggle/input/video-classification-data/Videos/Videos/videos-false-2' \nnon_kids_dir = '/kaggle/input/video-classification-data/non-made-for-kids/non-made-for-kids'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:06:37.254189Z","iopub.execute_input":"2024-12-30T06:06:37.254584Z","iopub.status.idle":"2024-12-30T06:06:37.259337Z","shell.execute_reply.started":"2024-12-30T06:06:37.254555Z","shell.execute_reply":"2024-12-30T06:06:37.258432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Functions to extract corresponding transcriptions and ground truths \n\ndef get_transcriptions_and_paths(vids_dir, ground_truths, transcriptions_path): \n\n    ids = os.listdir(vids_dir) \n\n    labels_df = pd.DataFrame({ \n        'IDs': ids, \n        'Labels': [ground_truths] * len(ids) \n    }) \n    \n    transcriptions_df = pd.read_csv(transcriptions_path) \n    \n    # Merging dataframes \n    df = pd.merge(labels_df, transcriptions_df, left_on='IDs', right_on='Video Id') \n\n    # Extracting transcriptions \n    ids = list(df['IDs']) \n    paths = [vids_dir + '/' + id for id in ids] \n    all_transcriptions = list(df['Transcription']) \n\n    # Extracting data from transcripts \n    transcriptions = [] \n    lengths = [] \n    for (i, id_) in enumerate(ids): \n        transcriptions.append(all_transcriptions[i].split(\"chunks\")[0]) \n        lengths.append(len(all_transcriptions[i].split(\"chunks\")[0])) \n\n    return paths, transcriptions, [ground_truths] * len(ids) ","metadata":{"id":"1ksUpR0ibKve","outputId":"6880ee21-f703-4761-dfcd-842e1f8b61d9","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:06:48.610315Z","iopub.execute_input":"2024-12-30T06:06:48.610723Z","iopub.status.idle":"2024-12-30T06:06:48.619842Z","shell.execute_reply.started":"2024-12-30T06:06:48.610692Z","shell.execute_reply":"2024-12-30T06:06:48.618581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading paths, transcriptions, and labels \n\ntrue1_paths, true1_transcriptions, true1_labels = get_transcriptions_and_paths(true1dir, 'Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/true-1-translated-transcriptions.csv') \ntrue2_paths, true2_transcriptions, true2_labels = get_transcriptions_and_paths(true2dir, 'Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/true-2-translated-transcriptions.csv') \nkids_paths, kids_transcriptions, kids_labels = get_transcriptions_and_paths(kids_dir, 'Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/made-for-kids_translated_transcriptions.csv') \n\nfalse1_paths, false1_transcriptions, false1_labels = get_transcriptions_and_paths(false1dir, 'Not Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/false-1-translated-transcriptions.csv') \nfalse2_paths, false2_transcriptions, false2_labels = get_transcriptions_and_paths(false2dir, 'Not Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/false-2-translated-transcriptions.csv') \nnon_kids_paths, non_kids_transcriptions, non_kids_labels = get_transcriptions_and_paths(non_kids_dir, 'Not Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/non-made-for-kids_translated_transcriptions.csv') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:06:50.707028Z","iopub.execute_input":"2024-12-30T06:06:50.707342Z","iopub.status.idle":"2024-12-30T06:06:50.930215Z","shell.execute_reply.started":"2024-12-30T06:06:50.707316Z","shell.execute_reply":"2024-12-30T06:06:50.929147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final combined list of paths and ground labels \n\npaths = [] \ntranscriptions = [] \nprimary_labels = [] \n\npaths.extend(false1_paths) \npaths.extend(true1_paths) \npaths.extend(false2_paths) \npaths.extend(true2_paths) \npaths.extend(non_kids_paths)\npaths.extend(kids_paths) \n\nprimary_labels.extend(false1_labels) \nprimary_labels.extend(true1_labels) \nprimary_labels.extend(false2_labels) \nprimary_labels.extend(true2_labels) \nprimary_labels.extend(non_kids_labels) \nprimary_labels.extend(kids_labels) \n\ntranscriptions.extend(false1_transcriptions) \ntranscriptions.extend(true1_transcriptions) \ntranscriptions.extend(false2_transcriptions) \ntranscriptions.extend(true2_transcriptions) \ntranscriptions.extend(non_kids_transcriptions) \ntranscriptions.extend(kids_transcriptions) \n\nvideo_ids = [path.split('/')[-1] for path in paths]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:06:53.343231Z","iopub.execute_input":"2024-12-30T06:06:53.343590Z","iopub.status.idle":"2024-12-30T06:06:53.351309Z","shell.execute_reply.started":"2024-12-30T06:06:53.343561Z","shell.execute_reply":"2024-12-30T06:06:53.350125Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(paths), len(primary_labels), len(transcriptions), len(video_ids))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:06:56.440055Z","iopub.execute_input":"2024-12-30T06:06:56.440365Z","iopub.status.idle":"2024-12-30T06:06:56.446348Z","shell.execute_reply.started":"2024-12-30T06:06:56.440341Z","shell.execute_reply":"2024-12-30T06:06:56.445104Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Extracting Images** ","metadata":{}},{"cell_type":"code","source":"def detect_scenes(video_path, threshold = 30):\n    \"\"\"Detect scenes in a video and return scene start and end frames.\"\"\"\n    scene_list = []\n    while len(scene_list) < 6 and threshold > 0:\n        threshold //= 2\n    \n        video = open_video(video_path)\n        scene_manager = SceneManager()\n        scene_manager.add_detector(ContentDetector(threshold=threshold))\n    \n        scene_manager.detect_scenes(video)\n        scene_list = scene_manager.get_scene_list()\n    \n    return scene_list\n\n\ndef get_top_n_longest_scenes(scene_list, n):\n    '''Return the top n longest scenes with start and end frame indices.'''\n    scene_durations = [(start, end - start) for start, end in scene_list]\n    scene_durations.sort(key=lambda x: x[1], reverse=True)\n\n    # Top n longest scenes with start and end frame indices\n    longest_scenes = [(start, start + duration) for start, duration in scene_durations[:n]]\n    return longest_scenes\n\n\ndef sort_scenes_by_frame(scenes_list):\n    '''Sort scenes by their start frame number.'''\n    sorted_scenes = sorted(scenes_list, key=lambda scene: scene[0].get_frames())\n    return sorted_scenes\n\n\ndef get_num_grids(video_path):\n    '''Get number of grids to be created'''\n    cap = cv2.VideoCapture(video_path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    duration = total_frames / fps\n\n    # Calculate number of grids based on the duration\n    duration = round(duration, 2)\n    if ((duration // 60) + 1) <= 5:\n        return int(((duration // 60) + 1))\n    else:\n        return 5\n\n\ndef extract_k_frames_from_scene(video_path, scene, k):\n    '''Extract k frames evenly spaced from each scene.'''\n    # Extract frame numbers from scene start and end\n    start_frame = scene[0].get_frames() + 1\n    end_frame = scene[1].get_frames() - 1\n\n    # Create k equally spaced frame indices within the scene's range\n    frame_indices = np.linspace(start_frame, end_frame, k, dtype=int)\n    \n    cap = cv2.VideoCapture(video_path)\n    frames = []\n\n    # Extract frames from calculated indices\n    for frame_no in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n        ret, frame = cap.read()\n        if ret:\n            frames.append(frame)\n    \n    cap.release()\n    return frames\n\n\ndef create_image_grid(frames, grid_size=(1000, 1000)):\n    '''Arrange 6 frames into a 3x2 grid and resize to the specified grid size.'''\n    # Ensure all frames have the same size for concatenation\n    frames = [cv2.resize(frame, (640, 360)) for frame in frames]  # Resize to a common size like 640x360\n    rows = [np.concatenate(frames[i:i+2], axis=1) for i in range(0, 6, 2)]\n    image_grid = np.concatenate(rows, axis=0)\n    \n    return np.array(Image.fromarray(image_grid).resize(grid_size))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:06:58.970851Z","iopub.execute_input":"2024-12-30T06:06:58.971232Z","iopub.status.idle":"2024-12-30T06:06:58.985661Z","shell.execute_reply.started":"2024-12-30T06:06:58.971202Z","shell.execute_reply":"2024-12-30T06:06:58.984442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_images(video_path, n=6):\n    ''' 1. Detect scenes\n        2. Get k; where k = num_grids\n        3. Get the 6 longest scenes\n        4. Sort scenes wrt frame numbers\n        5. Extract n * k frames\n        6. Create k image grids of n frames each\n     '''\n    scene_list = detect_scenes(video_path)\n    k = get_num_grids(video_path)\n    longest_scenes = get_top_n_longest_scenes(scene_list, n*k)\n    scenes = sort_scenes_by_frame(longest_scenes)\n\n    frames = []\n    for scene in scenes:\n        frames.extend(extract_k_frames_from_scene(video_path, scene, 1))\n\n    grids = []\n    for i in range(k):\n        start_idx = i * n\n        end_idx = start_idx + n\n        grid_frames = frames[start_idx:end_idx]\n        grid = create_image_grid(grid_frames, grid_size=(1000, 1000))\n        grids.append(grid)\n\n    return grids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:07:12.141332Z","iopub.execute_input":"2024-12-30T06:07:12.141734Z","iopub.status.idle":"2024-12-30T06:07:12.148366Z","shell.execute_reply.started":"2024-12-30T06:07:12.141704Z","shell.execute_reply":"2024-12-30T06:07:12.147142Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Defining Model** ","metadata":{}},{"cell_type":"code","source":"key_ = '' # add key here ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:07:19.460864Z","iopub.execute_input":"2024-12-30T06:07:19.461190Z","iopub.status.idle":"2024-12-30T06:07:19.466678Z","shell.execute_reply.started":"2024-12-30T06:07:19.461166Z","shell.execute_reply":"2024-12-30T06:07:19.465538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import google.generativeai as genai \nos.environ[\"API_KEY\"] = key_ \ngenai.configure(api_key=os.environ[\"API_KEY\"]) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:07:20.922468Z","iopub.execute_input":"2024-12-30T06:07:20.922801Z","iopub.status.idle":"2024-12-30T06:07:20.927793Z","shell.execute_reply.started":"2024-12-30T06:07:20.922775Z","shell.execute_reply":"2024-12-30T06:07:20.926732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = genai.GenerativeModel(\"gemini-1.5-flash\", system_instruction=\"You are an expert content moderator.\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:07:23.017668Z","iopub.execute_input":"2024-12-30T06:07:23.018014Z","iopub.status.idle":"2024-12-30T06:07:23.022917Z","shell.execute_reply.started":"2024-12-30T06:07:23.017986Z","shell.execute_reply":"2024-12-30T06:07:23.021933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LLM_Output(typing.TypedDict):\n    label: str\n    language: list[str]\n    response: str","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:07:23.518024Z","iopub.execute_input":"2024-12-30T06:07:23.518343Z","iopub.status.idle":"2024-12-30T06:07:23.523326Z","shell.execute_reply.started":"2024-12-30T06:07:23.518319Z","shell.execute_reply":"2024-12-30T06:07:23.522273Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Prompt** ","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"\n    A piece of content is child-directed if it meets any of these criteria:\n\n    Designed for Children:\n    The content is explicitly created with children as the intended audience, such as:\n      - Educational videos for kids.\n      - Child-friendly video games with cartoonish graphics and non-violent gameplay and story-telling\n      - Animated movies/clips suitable for family viewing\n      - Simple crafts, activities, or demonstrations aimed at children.\n\n    Child-Appealing Elements:\n    The content includes features commonly enjoyed by children, such as:\n    - Colorful, cartoonish, or animated visuals (e.g., animals, anthropomorphic characters, fantasy creatures).\n    - Light-hearted portrayals of themes like mischief or conflict, but without mature visuals, strong language, or adult humor.\n    - Whimsical, or playful themes.\n    - Non-violent and simplified gameplay or narratives.\n    - Content with rhythmic elements (e.g., chants, exclamations, or songs appealing to young viewers).\n\n    Important Considerations:\n    - Content can be child-directed even if it also appeals to older audiences.\n    - Gaming content is child-directed if it features family-friendly gameplay and cartoonish visuals.\n    - Movie/TV clips are child-directed if rated G/PG.\n    - Presence of mild peril or conflict is acceptable if presented appropriately.\n\n    Exclusions:\n    - Do not label content as \"Child Directed\" if it features significant:\n      - Dark, mature, or violent themes.\n      - Romantic subplots or adult humor.\n      - Complex dialogue or advanced vocabulary inappropriate for children.\n      - Songs that are not nursery rhymes or explicitly created for children.\n    - Recipe, DIY, or instructional videos unless simplified for children (e.g., \"Cooking for Kids\").\n\n    Instructions:\n    Labeling:\n    If the video meets the child-directed criteria label it as \"Child Directed\".\n    If it does not meet these criteria label it as \"Not Child Directed\".\n\n    Indicate the spoken language if any.\n    Provide a brief justification explaining why the video is considered child-directed or not.\n\n    Format the output in JSON.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:08:21.818533Z","iopub.execute_input":"2024-12-30T06:08:21.818916Z","iopub.status.idle":"2024-12-30T06:08:21.823671Z","shell.execute_reply.started":"2024-12-30T06:08:21.818890Z","shell.execute_reply":"2024-12-30T06:08:21.822553Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Running Model on Dataset** ","metadata":{"id":"w8esg7q8hSTW"}},{"cell_type":"code","source":"ids = []\npredicted_labels = []\nlanguages = []\nresponses = []\nground_truths = []\nremaining = []\n\nimg_dir = '/kaggle/working/Images'\nif not os.path.exists(img_dir):\n    os.makedirs(img_dir)\n\nfor i in range(len(paths)): \n    try:\n        contents_of_ad = os.listdir(paths[i]) \n        contents_of_ad.remove('audio.mp3') \n        video_path = paths[i] + '/' + contents_of_ad[0] \n        audio_path = paths[i] + '/audio.mp3' \n\n        # Extract multiple images representative of the video\n        images = get_images(video_path) \n\n        # Save each image returned by extract_images_of_frames\n        image_paths = []\n        for idx, img in enumerate(images):\n            # Convert NumPy array to PIL image\n            image = Image.fromarray(img)\n            \n            # Save the image to a file\n            image_name = f\"{video_ids[i]}_{idx + 1}.png\"\n            image_path = os.path.join(img_dir, image_name)\n            image.save(image_path)\n            image_paths.append(image_path)\n            \n        # Display the images\n        fig, axes = plt.subplots(1, len(images), figsize=(10, 3))\n        if len(images) == 1:\n            axes.imshow(images[0])\n            axes.axis('off')\n        else:\n            for ax, img in zip(axes, images):\n                ax.imshow(img)\n                ax.axis('off')\n\n        plt.tight_layout()\n        plt.show()\n\n        # Upload images and handle potential errors\n        uploaded_files = []\n        try:\n            for image_path in image_paths:\n                uploaded_file = genai.upload_file(path=image_path, resumable=False)\n                uploaded_files.append(uploaded_file)\n        except Exception as e:\n            print(f\"Error uploading images: {e}\")\n            remaining.append(video_ids[i])\n            continue\n\n        # Check if all images have uploaded\n        try:\n            for uploaded_file in uploaded_files:\n                while uploaded_file.state.name == \"PROCESSING\":\n                    print('.', end='')\n                    time.sleep(10)\n                    uploaded_file = genai.get_file(uploaded_file.name)\n                if uploaded_file.state.name == \"FAILED\":\n                    raise ValueError(uploaded_file.state.name)\n        except Exception as e:\n            print(f\"Error during image processing: {e}\")\n            remaining.append(video_ids[i])\n            continue\n\n        # Make inference with audio and image URIs\n        audio = transcriptions[i]\n        try: \n            inputs_ = [audio] \n            inputs_.extend(uploaded_files) \n            inputs_.extend([prompt]) \n            response = model.generate_content(inputs_,\n                                              generation_config=genai.GenerationConfig(\n                                                  response_mime_type=\"application/json\",\n                                                  response_schema=LLM_Output, \n                                                  temperature=0.0), \n                                              safety_settings={\n                                                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n                                                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n                                                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n                                                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n                                              })\n        except Exception as e:\n            print(f\"Error making inference: {e}\")\n            remaining.append(video_ids[i])\n            continue\n\n        # Wrap response.text access in try-except\n        try:\n            print(\"Completed for video number:\", i, ' ', video_ids[i])\n\n            dictionary = loads(response.text)\n            print('True Label:', primary_labels[i], 'Response:', dictionary)\n\n            ids.append(video_ids[i])\n            predicted_labels.append(dictionary['label'])\n            languages.append(dictionary['language'])\n            responses.append(dictionary['response'])\n            ground_truths.append(primary_labels[i]) \n            \n        except Exception as e:\n            print(f\"Error processing response.text: {e}\")\n            remaining.append(video_ids[i])\n            continue\n\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        remaining.append(video_ids[i])\n        continue\n\n    time.sleep(20)","metadata":{"id":"cSoT7-9DDknR","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:08:22.394504Z","iopub.execute_input":"2024-12-30T06:08:22.394825Z","execution_failed":"2024-12-30T06:12:12.120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T21:31:03.241782Z","iopub.execute_input":"2024-12-27T21:31:03.242279Z","iopub.status.idle":"2024-12-27T21:31:03.248925Z","shell.execute_reply.started":"2024-12-27T21:31:03.242245Z","shell.execute_reply":"2024-12-27T21:31:03.248050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# At the end, print remaining videos \n\nprint(\"Remaining videos with errors:\", remaining) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T20:20:24.318487Z","iopub.execute_input":"2024-12-28T20:20:24.318886Z","iopub.status.idle":"2024-12-28T20:20:24.324392Z","shell.execute_reply.started":"2024-12-28T20:20:24.318853Z","shell.execute_reply":"2024-12-28T20:20:24.323315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(responses)): \n    print('True Label: ', ground_truths[i], '\\tPrediction: ', predicted_labels[i]) ","metadata":{"id":"aqDlWLZcZJHL","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T20:20:25.416245Z","iopub.execute_input":"2024-12-28T20:20:25.416646Z","iopub.status.idle":"2024-12-28T20:20:25.422831Z","shell.execute_reply.started":"2024-12-28T20:20:25.416618Z","shell.execute_reply":"2024-12-28T20:20:25.421814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df = pd.DataFrame({\n    'Video Id': ids,\n    'Primary Label': ground_truths,\n    'Predicted Label': predicted_labels,\n    'Response': responses, \n    'Languages': languages \n})\n\nnew_df.head() ","metadata":{"id":"6JsiW7gbvxRA","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T20:20:26.202048Z","iopub.execute_input":"2024-12-28T20:20:26.202451Z","iopub.status.idle":"2024-12-28T20:20:26.222116Z","shell.execute_reply.started":"2024-12-28T20:20:26.202419Z","shell.execute_reply":"2024-12-28T20:20:26.220887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_dir = '/kaggle/working/results'\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T20:20:28.243246Z","iopub.execute_input":"2024-12-28T20:20:28.243678Z","iopub.status.idle":"2024-12-28T20:20:28.248624Z","shell.execute_reply.started":"2024-12-28T20:20:28.243642Z","shell.execute_reply":"2024-12-28T20:20:28.247450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df.to_csv('/kaggle/working/results/DAVSP_Gemini-1.5-Flash.csv', index=False)","metadata":{"id":"Tv1_5sfJwRfu","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T20:20:29.298115Z","iopub.execute_input":"2024-12-28T20:20:29.298509Z","iopub.status.idle":"2024-12-28T20:20:29.305677Z","shell.execute_reply.started":"2024-12-28T20:20:29.298476Z","shell.execute_reply":"2024-12-28T20:20:29.304792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Changing to binary lists \n\npredictions = [1 if pred == 'Not Child Directed' else 0 for pred in predicted_labels] \nground_truths = [1 if label == 'Not Child Directed' else 0 for label in ground_truths] ","metadata":{"id":"4hsLj8P8Mg-e","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T20:20:30.250996Z","iopub.execute_input":"2024-12-28T20:20:30.251376Z","iopub.status.idle":"2024-12-28T20:20:30.256231Z","shell.execute_reply.started":"2024-12-28T20:20:30.251346Z","shell.execute_reply":"2024-12-28T20:20:30.255100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Obtaining classification report \nfrom sklearn.metrics import classification_report \n\nreport = classification_report(ground_truths, predictions) \nprint(report) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T20:20:31.096205Z","iopub.execute_input":"2024-12-28T20:20:31.096591Z","iopub.status.idle":"2024-12-28T20:20:31.266264Z","shell.execute_reply.started":"2024-12-28T20:20:31.096564Z","shell.execute_reply":"2024-12-28T20:20:31.265366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(ground_truths, predictions)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Child Directed', 'Child Directed'], yticklabels=['Not Child Directed', 'Child Directed'])\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T20:20:31.864369Z","iopub.execute_input":"2024-12-28T20:20:31.864756Z","iopub.status.idle":"2024-12-28T20:20:32.147778Z","shell.execute_reply.started":"2024-12-28T20:20:31.864728Z","shell.execute_reply":"2024-12-28T20:20:32.146681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}