{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10328861,"sourceType":"datasetVersion","datasetId":6395471},{"sourceId":10349036,"sourceType":"datasetVersion","datasetId":6296913},{"sourceId":10358109,"sourceType":"datasetVersion","datasetId":5987034},{"sourceId":10391050,"sourceType":"datasetVersion","datasetId":5684885},{"sourceId":10550838,"sourceType":"datasetVersion","datasetId":6411015}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Imports","metadata":{"id":"exmY-abTefFW"}},{"cell_type":"code","source":"!pip install openai\n!pip install ffmpeg-python\n!pip install av\n!pip install scenedetect","metadata":{"id":"Bi3c0wLhh-Rf","outputId":"44c463e5-7c4c-45cf-daca-75620249739c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport time\nimport json\nimport torch\nimport random\nimport ffmpeg\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom glob import glob\nimport soundfile as sf\nfrom openai import OpenAI\nfrom json import loads,dumps\nimport matplotlib.pyplot as plt\n#from pydub import AudioSegment\nfrom scipy.signal import resample\nimport typing_extensions as typing\nfrom google.generativeai.types import HarmCategory, HarmBlockThreshold\nfrom scenedetect import open_video, VideoStreamCv2, SceneManager\nfrom scenedetect.detectors import ContentDetector","metadata":{"id":"iNg0gXOfD_hY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extracting all required ids\n\nlabels_df = pd.read_csv('/kaggle/input/youtube-data/all_unique_codes3.csv')\ntranscriptions_df = pd.read_csv('/kaggle/input/youtube-data/Translated-transcriptions.csv')\n\n# Merging on different key names\ndf = pd.merge(labels_df, transcriptions_df, left_on='Video link', right_on='Video Id')\ndf.head() ","metadata":{"id":"1ksUpR0ibKve","outputId":"6880ee21-f703-4761-dfcd-842e1f8b61d9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[(df['Primary Tag'] == 'irrelevant') | (df['Primary Tag'] == 'inappropriate') | (df['Primary Tag'] == 'child directed')] \ndf['Primary Tag'].value_counts() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extracting video ids and primary labels\n\nvideo_ids = list(df['Video link'])\nprimary_labels = list(df['Primary Tag'])\nall_transcriptions = list(df['Transcription'])","metadata":{"id":"oR7rNEX3rl2I","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(video_ids)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extracting data from transcripts\n\ntranscriptions = []\nlengths = []\n\nfor (i, id_) in enumerate(video_ids):\n    transcriptions.append(all_transcriptions[i].split(\"chunks\")[0])\n    lengths.append(len(all_transcriptions[i].split(\"chunks\")[0]))","metadata":{"id":"_7p-efT9sNMf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os \n\navailable_ad_ids = os.listdir('/kaggle/input/youtube-data/Ads/Ads') \nlen(available_ad_ids) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Mapping Ad Durations to Number of Images / Frames** ","metadata":{}},{"cell_type":"code","source":"durations_in_seconds = [] \nnum_frames = [] \n\nfor i in range(len(video_ids)): \n\n    contents_of_ad = os.listdir('/kaggle/input/youtube-data/Ads/Ads/' + video_ids[i]) \n    contents_of_ad.remove('audio.mp3') \n    video_path = '/kaggle/input/youtube-data/Ads/Ads/' + video_ids[i] + '/' + contents_of_ad[0] \n\n    cap = cv2.VideoCapture(video_path) \n\n    # Get the frames per second (fps) of the video\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    # Get the total number of frames in the video\n    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    # Calculate the duration in seconds\n    duration = total_frames / fps \n    \n    durations_in_seconds.append(round(duration, 2)) \n    num_frames.append(total_frames) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculating number of images needed for each ad \n\n# < 1 minute: 1 image \n# 1 - 2 minutes: 2 images \n# 2 - 3 minutes: 3 images \n# 3 - 4 minutes: 4 images \n# > 4 minutes: 5 images \n\nnum_images = [int(((x // 60) + 1)) if ((x // 60) + 1) <= 5 else 5 for x in durations_in_seconds] ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\n\n# Count the frequency of each number\nfrequency = Counter(num_images)\nvalues = list(frequency.keys())\ncounts = list(frequency.values())\n\nplt.bar(values, counts)\n\nplt.xlabel(\"Number of Grids for Ad\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Number of grids for each ad\")\n\n# Show the plot\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Defining Model** ","metadata":{"id":"MyTbSTVcehXl"}},{"cell_type":"code","source":"apikey = \"\" # add key here ","metadata":{"id":"d4BaWth9kkKa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"client = OpenAI(api_key=apikey) ","metadata":{"id":"WgJcICGYd2Q8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Prompt** ","metadata":{"id":"V2kv113nelLF"}},{"cell_type":"code","source":"prompt = \"\"\" A video can be considered inappropriate for children if it contains physical violence (cartoonish, realistic, inclusion of gore),\n  interpersonal violence (such as bullying, pranks, meanness, humor at expense of others, belittling ,controlling, talking down to, or manipulating others),\n  self-harm or suicide,\n  elements of scariness (horror, scary themes, creepy or suspenseful themes, loud and violent content, zombies, skeletons, scary masks, scary clowns, blood, fire in dangerous settings,\n  mentally distressing scenes (car crashes, medical procedures etc),\n  extreme stunts (depictions of life endangering or high-risk physical activities/challenges),\n  sexual content (innuendos, overt sexual behavior, nudity, sexual themes, sexual suggestiveness),\n  discrimination (based on race/ethnicity/gender/ability),\n  offensive language (from mild words such as \"idiot\" and \"stupid\" to severe swearing, hate speech or signs),\n  drinking, drugs, smoking, gambling, crude humour,\n  inappropriate clothing (with significant skin exposure),\n  death,\n  intimacy (sensual physical contact, marriage, romance, dating and relationships, apparent romantic conduct between subjects/actors).\n  romantic songs and music.\n\n  You have been given a image grid (3 x 2) representing a video and it's audio transcription.\n  Consider that you are a Parent. Based on both the visual content and audio transcription, is the video ad provided inappropriate for children under 9 based on this definition?\n  \n  If it is, label it as \"inappropriate\".\n  If it is not, label it as \"appropriate\".\n\n  Justify your conclusion.\n\n  Also, mention what language is being spoken if there is any.\n\n  Make the outputs in JSON format.\n\n  \"\"\"","metadata":{"id":"3UX0f8bHemQW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Output Schema** ","metadata":{}},{"cell_type":"code","source":"output_schema = {\n  \"type\": \"object\",\n  \"properties\": {\n    \"label\": {\n      \"type\": \"string\",\n      \"enum\": [\"inappropriate\", \"appropriate\"]\n    },\n    \"justification\": {\n      \"type\": \"string\",\n      \"minLength\": 10\n    }, \n    \"languages\": {\n        \"type\": \"array\", \n        \"items\": {\n            \"type\": \"string\" \n        }\n        \n    }\n  },\n  \"required\": [\"label\", \"justification\", \"languages\"]\n} ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Setting Up Image Data** ","metadata":{}},{"cell_type":"code","source":"import base64\nimport requests\n\ndef encode_image(image_path):\n    \"\"\"\n    Encodes an image at the given path to a base64 string.\n    \"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\ndef classify_video_with_images(text_input, audio_transcription, image_paths):\n    \"\"\"\n    Sends text, audio transcription, and multiple images to the API for classification.\n    \n    Parameters:\n    - text_input: str, the input text.\n    - audio_transcription: str, transcription of the audio.\n    - image_paths: list of str, paths to the images.\n\n    Returns:\n    - response: The API response.\n    \"\"\"\n    # Encode all images to base64\n    encoded_images = [\n        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(path)}\"}}\n        for path in image_paths\n    ]\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {apikey}\"\n    }\n\n    payload = {\n        \"model\": \"gpt-4o-2024-08-06\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": text_input},\n                    {\"type\": \"text\", \"text\": 'Audio transcription: ' + audio_transcription},\n                ] + encoded_images  # Append all encoded images to the message content\n            }\n        ],\n        \"response_format\": {\n            \"type\": \"json_schema\",\n            \"json_schema\": {\n                \"name\": \"output_schema\",\n                \"schema\": output_schema\n            }\n        },\n        \"max_tokens\": 300\n    }\n\n    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n\n    return response\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def detect_scenes(video_path, threshold = 30):\n    \"\"\"Detect scenes in a video and return scene start and end frames.\"\"\"\n    scene_list = []\n    while len(scene_list) < 6 and threshold > 0:\n        threshold //= 2\n    \n        video = open_video(video_path)\n        scene_manager = SceneManager()\n        scene_manager.add_detector(ContentDetector(threshold=threshold))\n    \n        scene_manager.detect_scenes(video)\n        scene_list = scene_manager.get_scene_list()\n    \n    return scene_list\n\n\ndef get_top_n_longest_scenes(scene_list, n):\n    '''Return the top n longest scenes with start and end frame indices.'''\n    scene_durations = [(start, end - start) for start, end in scene_list]\n    scene_durations.sort(key=lambda x: x[1], reverse=True)\n\n    # Top n longest scenes with start and end frame indices\n    longest_scenes = [(start, start + duration) for start, duration in scene_durations[:n]]\n    return longest_scenes\n\ndef sort_scenes_by_frame(scenes_list):\n    '''Sort scenes by their start frame number.'''\n    sorted_scenes = sorted(scenes_list, key=lambda scene: scene[0].get_frames())\n    return sorted_scenes\n\n\ndef get_num_grids(video_path):\n    '''Get number of grids to be created'''\n    cap = cv2.VideoCapture(video_path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    duration = total_frames / fps\n\n    # Calculate number of grids based on the duration\n    duration = round(duration, 2)\n    if ((duration // 60) + 1) <= 5:\n        return int(((duration // 60) + 1))\n    else:\n        return 5\n\ndef extract_k_frames_from_scene(video_path, scene, k):\n    '''Extract k frames evenly spaced from each scene.'''\n    # Extract frame numbers from scene start and end\n    start_frame = scene[0].get_frames() + 1\n    end_frame = scene[1].get_frames() - 1\n\n    # Create k equally spaced frame indices within the scene's range\n    frame_indices = np.linspace(start_frame, end_frame, k, dtype=int)\n    \n    cap = cv2.VideoCapture(video_path)\n    frames = []\n\n    # Extract frames from calculated indices\n    for frame_no in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n        ret, frame = cap.read()\n        if ret:\n            frames.append(frame)\n    \n    cap.release()\n    return frames\n\n\ndef create_image_grid(frames, grid_size=(1000, 1000)):\n    '''Arrange 6 frames into a 3x2 grid and resize to the specified grid size.'''\n    # Ensure all frames have the same size for concatenation\n    frames = [cv2.resize(frame, (640, 360)) for frame in frames]  # Resize to a common size like 640x360\n    rows = [np.concatenate(frames[i:i+2], axis=1) for i in range(0, 6, 2)]\n    image_grid = np.concatenate(rows, axis=0)\n    \n    return np.array(Image.fromarray(image_grid).resize(grid_size))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_images(video_path, n=6):\n    ''' 1. Detect scenes\n        2. Get k; where k = num_grids\n        3. Get the 6 longest scenes\n        4. Sort scenes wrt frame numbers\n        5. Extract n * k frames\n        6. Create k image grids of n frames each\n     '''\n    scene_list = detect_scenes(video_path)\n    k = get_num_grids(video_path)\n    #k = 1 # For Single Grid of Major Scene Frames\n    longest_scenes = get_top_n_longest_scenes(scene_list, n*k)\n    scenes = sort_scenes_by_frame(longest_scenes)\n\n    frames = []\n    for scene in scenes:\n        frames.extend(extract_k_frames_from_scene(video_path, scene, 1))\n\n    grids = []\n    for i in range(k):\n        start_idx = i * n\n        end_idx = start_idx + n\n        grid_frames = frames[start_idx:end_idx]\n        grid = create_image_grid(grid_frames, grid_size=(1000, 1000))\n        grids.append(grid)\n\n    return grids","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Running Model** ","metadata":{"id":"w8esg7q8hSTW"}},{"cell_type":"code","source":"ids = []\nlabels = []\nresponses = []\npredicted_labels = [] \nlanguages = [] \nremaining = [] \n\nimg_dir = '/kaggle/working/Images'\nif not os.path.exists(img_dir):\n    os.makedirs(img_dir)\n\nfor i in range(len(video_ids)):\n\n    if video_ids[i] in available_ad_ids:\n\n        contents_of_ad = os.listdir('/kaggle/input/youtube-data/Ads/Ads/' + video_ids[i]) \n        contents_of_ad.remove('audio.mp3') \n        video_path = '/kaggle/input/youtube-data/Ads/Ads/' + video_ids[i] + '/' + contents_of_ad[0] \n\n        print('\\n', i, video_path)\n        \n        try:\n            # Extract multiple images representative of the video\n            images = get_images(video_path)\n\n            # Save each image returned by extract_images_of_frames\n            image_paths = []\n            for idx, img in enumerate(images):\n                # Convert NumPy array to PIL image\n                image = Image.fromarray(img)\n                \n                # Save the image to a file\n                image_name = f\"{video_ids[i]}_{idx + 1}.png\"\n                image_path = os.path.join(img_dir, image_name)\n                image.save(image_path)\n                image_paths.append(image_path)\n                \n            # Display the images\n            fig, axes = plt.subplots(1, len(images), figsize=(10, 3))\n            if len(images) == 1:\n                axes.imshow(images[0])\n                axes.axis('off')\n            else:\n                for ax, img in zip(axes, images):\n                    ax.imshow(img)\n                    ax.axis('off')\n\n            plt.tight_layout()\n            plt.show()\n\n            audio_transcription = transcriptions[i] \n\n            try: \n                \n                classification_response = classify_video_with_images(prompt, audio_transcription, image_paths)\n            \n                temp_id = video_ids[i] \n                temp_label = primary_labels[i] \n                temp_predicted_label = json.loads(classification_response.json()['choices'][0]['message']['content']).get('label') \n                temp_response = classification_response.json()['choices'][0]['message']['content'] \n                temp_languages = json.loads(classification_response.json()['choices'][0]['message']['content']).get('languages') \n\n                ids.append(temp_id)\n                labels.append(temp_label)\n                predicted_labels.append(temp_predicted_label)\n                responses.append(temp_response)\n                languages.append(temp_languages) \n\n                print('\\nPrimary Label:', primary_labels[i], '. Response:', classification_response.json()['choices'][0]['message']['content'])\n\n            except Exception as e: \n                print('\\nClassification failed for ', i, 'Error:', str(e)) \n                remaining.append(video_ids[i])\n\n        except Exception as e:\n            print('\\nImage extraction failed for ', i, 'Error:', str(e))\n            remaining.append(video_ids[i])\n\n        time.sleep(20) ","metadata":{"id":"fAc-89lDDknS","outputId":"df6ab726-c98f-4d74-b330-5dcbd5eadf36","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for x in remaining: \n    print('\\'' + x + '\\',') ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"remaining ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(responses)): \n    print('True Label: ', labels[i], '\\tPrediction: ', predicted_labels[i]) ","metadata":{"id":"aqDlWLZcZJHL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df = pd.DataFrame({\n    'Video Id': ids,\n    'Primary Label': labels,\n    'Predicted Label': predicted_labels,\n    'Languages': languages, \n    'Response': responses\n})\n\nnew_df.head() ","metadata":{"id":"6JsiW7gbvxRA","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_dir = '/kaggle/working/results'\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df.to_csv('/kaggle/working/results/DAVSP_GPT-4o.csv', index=False) ","metadata":{"id":"Tv1_5sfJwRfu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Changing to binary lists \n\npredictions = [1 if pred == 'inappropriate' else 0 for pred in predicted_labels] \nground_truths = [1 if label == 'inappropriate' else 0 for label in labels] ","metadata":{"id":"4hsLj8P8Mg-e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Obtaining classification report \nfrom sklearn.metrics import classification_report \n\nreport = classification_report(ground_truths, predictions) \nprint(report) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(ground_truths, predictions)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Appropriate', 'Inapproriate'], yticklabels=['Appropriate', 'Inapproriate'])\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}