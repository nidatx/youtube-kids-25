{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10328861,"sourceType":"datasetVersion","datasetId":6395471},{"sourceId":10349036,"sourceType":"datasetVersion","datasetId":6296913},{"sourceId":10358109,"sourceType":"datasetVersion","datasetId":5987034},{"sourceId":10549716,"sourceType":"datasetVersion","datasetId":6179624},{"sourceId":10550838,"sourceType":"datasetVersion","datasetId":6411015}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"id":"exmY-abTefFW"}},{"cell_type":"code","source":"!pip install anthropic\n!pip install ffmpeg-python\n!pip install av\n!pip install scenedetect","metadata":{"id":"Bi3c0wLhh-Rf","outputId":"44c463e5-7c4c-45cf-daca-75620249739c","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:53:28.774602Z","iopub.execute_input":"2025-01-10T08:53:28.774910Z","iopub.status.idle":"2025-01-10T08:53:48.043913Z","shell.execute_reply.started":"2025-01-10T08:53:28.774863Z","shell.execute_reply":"2025-01-10T08:53:48.042636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport anthropic\nfrom json import loads,dumps\nimport time\nfrom glob import glob\nimport json\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:54:39.661572Z","iopub.execute_input":"2025-01-10T08:54:39.661927Z","iopub.status.idle":"2025-01-10T08:54:41.564886Z","shell.execute_reply.started":"2025-01-10T08:54:39.661900Z","shell.execute_reply":"2025-01-10T08:54:41.563969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport torch\nimport random\nimport ffmpeg\nimport warnings\nimport numpy as np\nfrom PIL import Image\nfrom glob import glob\nimport soundfile as sf\nimport matplotlib.pyplot as plt\n#from pydub import AudioSegment\nfrom scipy.signal import resample\nimport typing_extensions as typing\nfrom scenedetect import open_video,  VideoStreamCv2, SceneManager\nfrom scenedetect.detectors import ContentDetector","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:54:44.380385Z","iopub.execute_input":"2025-01-10T08:54:44.380910Z","iopub.status.idle":"2025-01-10T08:54:50.654401Z","shell.execute_reply.started":"2025-01-10T08:54:44.380852Z","shell.execute_reply":"2025-01-10T08:54:50.653540Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"limit = 5000 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:54:52.948380Z","iopub.execute_input":"2025-01-10T08:54:52.948970Z","iopub.status.idle":"2025-01-10T08:54:52.952781Z","shell.execute_reply.started":"2025-01-10T08:54:52.948943Z","shell.execute_reply":"2025-01-10T08:54:52.951657Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"# Setting up video directories \n\ntrue1dir = '/kaggle/input/video-classification-data/Videos/Videos/videos-true' \ntrue2dir = '/kaggle/input/video-classification-data/Videos/Videos/videos-true-2' \nkids_dir = '/kaggle/input/video-classification-data/Made-For-Kids/Made-For-Kids'\n\nfalse1dir = '/kaggle/input/video-classification-data/Videos/Videos/videos-false' \nfalse2dir = '/kaggle/input/video-classification-data/Videos/Videos/videos-false-2' \nnon_kids_dir = '/kaggle/input/video-classification-data/Non Made For Kids/Non Made For Kids'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:54:55.895515Z","iopub.execute_input":"2025-01-10T08:54:55.895833Z","iopub.status.idle":"2025-01-10T08:54:55.899944Z","shell.execute_reply.started":"2025-01-10T08:54:55.895811Z","shell.execute_reply":"2025-01-10T08:54:55.898956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Functions to extract corresponding transcriptions and ground truths \n\ndef get_transcriptions_and_paths(vids_dir, ground_truths, transcriptions_path): \n\n    ids = os.listdir(vids_dir) \n\n    labels_df = pd.DataFrame({ \n        'IDs': ids, \n        'Labels': [ground_truths] * len(ids) \n    }) \n    \n    transcriptions_df = pd.read_csv(transcriptions_path) \n    \n    # Merging dataframes \n    df = pd.merge(labels_df, transcriptions_df, left_on='IDs', right_on='Video Id') \n\n    # Extracting transcriptions \n    ids = list(df['IDs']) \n    paths = [vids_dir + '/' + id for id in ids] \n    all_transcriptions = list(df['Transcription']) \n\n    # Extracting data from transcripts \n    transcriptions_temp = [] \n    lengths = [] \n    for (i, id_) in enumerate(ids): \n        transcriptions_temp.append(all_transcriptions[i].split(\"chunks\")[0]) \n        lengths.append(len(all_transcriptions[i].split(\"chunks\")[0])) \n\n    # setting limit on transcription length \n    transcriptions = [x[:limit] + '...' if len(x) > limit else x for x in transcriptions_temp] \n\n    return paths, transcriptions, [ground_truths] * len(ids) ","metadata":{"id":"1ksUpR0ibKve","outputId":"6880ee21-f703-4761-dfcd-842e1f8b61d9","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:54:58.485790Z","iopub.execute_input":"2025-01-10T08:54:58.486173Z","iopub.status.idle":"2025-01-10T08:54:58.494647Z","shell.execute_reply.started":"2025-01-10T08:54:58.486145Z","shell.execute_reply":"2025-01-10T08:54:58.493660Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading paths, transcriptions, and labels \n\ntrue1_paths, true1_transcriptions, true1_labels = get_transcriptions_and_paths(true1dir, 'Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/true-1-translated-transcriptions.csv') \ntrue2_paths, true2_transcriptions, true2_labels = get_transcriptions_and_paths(true2dir, 'Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/true-2-translated-transcriptions.csv') \nkids_paths, kids_transcriptions, kids_labels = get_transcriptions_and_paths(kids_dir, 'Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/made-for-kids_translated_transcriptions.csv') \n\nfalse1_paths, false1_transcriptions, false1_labels = get_transcriptions_and_paths(false1dir, 'Not Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/false-1-translated-transcriptions.csv') \nfalse2_paths, false2_transcriptions, false2_labels = get_transcriptions_and_paths(false2dir, 'Not Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/false-2-translated-transcriptions.csv') \nnon_kids_paths, non_kids_transcriptions, non_kids_labels = get_transcriptions_and_paths(non_kids_dir, 'Not Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/non-made-for-kids_translated_transcriptions.csv') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:55:01.002472Z","iopub.execute_input":"2025-01-10T08:55:01.002830Z","iopub.status.idle":"2025-01-10T08:55:01.643811Z","shell.execute_reply.started":"2025-01-10T08:55:01.002804Z","shell.execute_reply":"2025-01-10T08:55:01.642979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final combined list of paths and ground labels \n\npaths = [] \ntranscriptions = [] \nprimary_labels = [] \n\npaths.extend(false1_paths) \npaths.extend(true1_paths) \npaths.extend(false2_paths) \npaths.extend(true2_paths) \npaths.extend(non_kids_paths)\npaths.extend(kids_paths) \n\nprimary_labels.extend(false1_labels) \nprimary_labels.extend(true1_labels) \nprimary_labels.extend(false2_labels) \nprimary_labels.extend(true2_labels) \nprimary_labels.extend(non_kids_labels) \nprimary_labels.extend(kids_labels) \n\ntranscriptions.extend(false1_transcriptions) \ntranscriptions.extend(true1_transcriptions) \ntranscriptions.extend(false2_transcriptions) \ntranscriptions.extend(true2_transcriptions) \ntranscriptions.extend(non_kids_transcriptions) \ntranscriptions.extend(kids_transcriptions) \n\nvideo_ids = [path.split('/')[-1] for path in paths]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:55:04.078771Z","iopub.execute_input":"2025-01-10T08:55:04.079144Z","iopub.status.idle":"2025-01-10T08:55:04.086480Z","shell.execute_reply.started":"2025-01-10T08:55:04.079122Z","shell.execute_reply":"2025-01-10T08:55:04.085219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(paths), len(primary_labels), len(transcriptions), len(video_ids)) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:55:04.357381Z","iopub.execute_input":"2025-01-10T08:55:04.357686Z","iopub.status.idle":"2025-01-10T08:55:04.363245Z","shell.execute_reply.started":"2025-01-10T08:55:04.357665Z","shell.execute_reply":"2025-01-10T08:55:04.362372Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Extracting Images** ","metadata":{}},{"cell_type":"code","source":"def detect_scenes(video_path, threshold = 30):\n    \"\"\"Detect scenes in a video and return scene start and end frames.\"\"\"\n    scene_list = []\n    while len(scene_list) < 6 and threshold > 0:\n        threshold //= 2\n    \n        video = open_video(video_path)\n        scene_manager = SceneManager()\n        scene_manager.add_detector(ContentDetector(threshold=threshold))\n    \n        scene_manager.detect_scenes(video)\n        scene_list = scene_manager.get_scene_list()\n    \n    return scene_list\n\n\ndef get_top_n_longest_scenes(scene_list, n):\n    '''Return the top n longest scenes with start and end frame indices.'''\n    scene_durations = [(start, end - start) for start, end in scene_list]\n    scene_durations.sort(key=lambda x: x[1], reverse=True)\n\n    # Top n longest scenes with start and end frame indices\n    longest_scenes = [(start, start + duration) for start, duration in scene_durations[:n]]\n    return longest_scenes\n\ndef sort_scenes_by_frame(scenes_list):\n    '''Sort scenes by their start frame number.'''\n    sorted_scenes = sorted(scenes_list, key=lambda scene: scene[0].get_frames())\n    return sorted_scenes\n\n\ndef get_num_grids(video_path):\n    '''Get number of grids to be created'''\n    cap = cv2.VideoCapture(video_path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    duration = total_frames / fps\n\n    # Calculate number of grids based on the duration\n    duration = round(duration, 2)\n    if ((duration // 60) + 1) <= 5:\n        return int(((duration // 60) + 1))\n    else:\n        return 5\n\ndef extract_k_frames_from_scene(video_path, scene, k):\n    '''Extract k frames evenly spaced from each scene.'''\n    # Extract frame numbers from scene start and end\n    start_frame = scene[0].get_frames() + 1\n    end_frame = scene[1].get_frames() - 1\n\n    # Create k equally spaced frame indices within the scene's range\n    frame_indices = np.linspace(start_frame, end_frame, k, dtype=int)\n    \n    cap = cv2.VideoCapture(video_path)\n    frames = []\n\n    # Extract frames from calculated indices\n    for frame_no in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n        ret, frame = cap.read()\n        if ret:\n            frames.append(frame)\n    \n    cap.release()\n    return frames\n\n\ndef create_image_grid(frames, grid_size=(1000, 1000)):\n    '''Arrange 6 frames into a 3x2 grid and resize to the specified grid size.'''\n    # Ensure all frames have the same size for concatenation\n    frames = [cv2.resize(frame, (640, 360)) for frame in frames]  # Resize to a common size like 640x360\n    rows = [np.concatenate(frames[i:i+2], axis=1) for i in range(0, 6, 2)]\n    image_grid = np.concatenate(rows, axis=0)\n    \n    return np.array(Image.fromarray(image_grid).resize(grid_size))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:55:07.892687Z","iopub.execute_input":"2025-01-10T08:55:07.893043Z","iopub.status.idle":"2025-01-10T08:55:07.902576Z","shell.execute_reply.started":"2025-01-10T08:55:07.893019Z","shell.execute_reply":"2025-01-10T08:55:07.901792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_images(video_path, n=6):\n    ''' 1. Detect scenes\n        2. Get k; where k = num_grids\n        3. Get the 6 longest scenes\n        4. Sort scenes wrt frame numbers\n        5. Extract n * k frames\n        6. Create k image grids of n frames each\n     '''\n    scene_list = detect_scenes(video_path)\n    k = get_num_grids(video_path)\n    #k = 1 # For Single Grid of Major Scene Frames\n    longest_scenes = get_top_n_longest_scenes(scene_list, n*k)\n    scenes = sort_scenes_by_frame(longest_scenes)\n\n    frames = []\n    for scene in scenes:\n        frames.extend(extract_k_frames_from_scene(video_path, scene, 1))\n\n    grids = []\n    for i in range(k):\n        start_idx = i * n\n        end_idx = start_idx + n\n        grid_frames = frames[start_idx:end_idx]\n        grid = create_image_grid(grid_frames, grid_size=(1000, 1000))\n        grids.append(grid)\n\n    return grids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:55:11.535716Z","iopub.execute_input":"2025-01-10T08:55:11.536089Z","iopub.status.idle":"2025-01-10T08:55:11.543171Z","shell.execute_reply.started":"2025-01-10T08:55:11.536064Z","shell.execute_reply":"2025-01-10T08:55:11.541710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_images_2(video_path, n = 6):\n    ''' \n    Caters the videos where <= 6 scenes are extracted\n    '''\n\n    # Step 1: Detect scenes\n    scene_list = detect_scenes(video_path)\n    if not scene_list:\n        return []  # Handle case where no scenes are detected\n    \n    # Step 2: Get the number of grids (k)\n    k = get_num_grids(video_path)\n    \n    # Total number of frames needed\n    total_frames_needed = n * k\n    available_scenes = len(scene_list)\n\n    if available_scenes == 0:\n        return []  # Handle case where no scenes are available for frame extraction\n    \n    # Step 3: Adjust the number of frames extracted per scene\n    frames_per_scene = total_frames_needed // available_scenes\n    remaining_frames = total_frames_needed % available_scenes\n\n    if available_scenes == 1:\n        frames_per_scene = total_frames_needed  # Assign all frames to the single scene\n        remaining_frames = 0  # No remaining frames\n\n    # Extract frames evenly across all scenes\n    frames = []\n    for i, scene in enumerate(scene_list):\n        num_frames = frames_per_scene + (1 if i < remaining_frames else 0)\n        frames.extend(extract_k_frames_from_scene(video_path, scene, num_frames))\n\n    # Ensure we have exactly total_frames_needed frames\n    frames = frames[:total_frames_needed]\n    \n    # Step 4: Create image grids\n    grids = []\n    if len(frames) < n:  # If fewer frames are available than needed\n        # Repeat the available frames until there are enough\n        frames = frames * (n // len(frames)) + frames[:(n % len(frames))]\n\n    # Create the grids\n    for i in range(k):\n        start_idx = i * n\n        end_idx = start_idx + n\n        grid_frames = frames[start_idx:end_idx]\n        if len(grid_frames) > 0:  # Ensure we have frames to create the grid\n            grid = create_image_grid(grid_frames, grid_size=(1000, 1000))\n            grids.append(grid)\n    \n    return grids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:55:13.204531Z","iopub.execute_input":"2025-01-10T08:55:13.204862Z","iopub.status.idle":"2025-01-10T08:55:13.211741Z","shell.execute_reply.started":"2025-01-10T08:55:13.204840Z","shell.execute_reply":"2025-01-10T08:55:13.210525Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Defining Model** ","metadata":{}},{"cell_type":"code","source":"os.environ[\"ANTHROPIC_API_KEY\"] = \"\" # add key here ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:55:15.415051Z","iopub.execute_input":"2025-01-10T08:55:15.415406Z","iopub.status.idle":"2025-01-10T08:55:15.419470Z","shell.execute_reply.started":"2025-01-10T08:55:15.415382Z","shell.execute_reply":"2025-01-10T08:55:15.418541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"client = anthropic.Anthropic() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:55:16.189243Z","iopub.execute_input":"2025-01-10T08:55:16.189528Z","iopub.status.idle":"2025-01-10T08:55:16.245934Z","shell.execute_reply.started":"2025-01-10T08:55:16.189507Z","shell.execute_reply":"2025-01-10T08:55:16.245105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import base64\nimport httpx\n\n# Function to encode the image\n\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file: \n        image_media_type = \"image/png\" \n        image_data = base64.b64encode(image_file.read()).decode('utf-8') \n        return image_media_type, image_data ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:55:18.058737Z","iopub.execute_input":"2025-01-10T08:55:18.059091Z","iopub.status.idle":"2025-01-10T08:55:18.064233Z","shell.execute_reply.started":"2025-01-10T08:55:18.059069Z","shell.execute_reply":"2025-01-10T08:55:18.063088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def classify_video_with_images(text_input, audio_transcription, image_paths):\n    \n    image_contents = [\n        {\n            \"type\": \"image\",\n            \"source\": {\n                \"type\": \"base64\",\n                \"media_type\": img_media_type,\n                \"data\": img_data,\n            }\n        }\n        for image_path in image_paths\n        for img_media_type, img_data in [encode_image(image_path)]\n    ]\n    \n    # Prepare the payload with images and text\n    response = client.messages.create(\n        model=\"claude-3-5-sonnet-20240620\",\n        system=\"You are a content classification assistant that evaluates image frames and audio transcriptions from videos according to strict guidelines.\",\n        max_tokens=1024,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    *image_contents,  # Unpack the list of images\n                    {\n                        \"type\": \"text\",\n                        \"text\": text_input\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": audio_transcription\n                    }\n                ],\n            }\n        ]\n    )\n\n    return response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:55:18.287351Z","iopub.execute_input":"2025-01-10T08:55:18.287749Z","iopub.status.idle":"2025-01-10T08:55:18.293828Z","shell.execute_reply.started":"2025-01-10T08:55:18.287716Z","shell.execute_reply":"2025-01-10T08:55:18.292454Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Prompt** ","metadata":{}},{"cell_type":"code","source":"# Create the prompt.\nprompt = \"\"\"\n    A piece of content is child-directed if it meets any of these criteria:\n\n    Designed for Children:\n    The content is explicitly created with children as the intended audience, such as:\n      - Educational videos for kids.\n      - Child-friendly video games with cartoonish graphics and non-violent gameplay and story-telling\n      - Animated movies/clips suitable for family viewing\n      - Simple crafts, activities, or demonstrations aimed at children.\n\n    Child-Appealing Elements:\n    The content includes features commonly enjoyed by children, such as:\n    - Colorful, cartoonish, or animated visuals (e.g., animals, anthropomorphic characters, fantasy creatures).\n    - Light-hearted portrayals of themes like mischief or conflict, but without mature visuals, strong language, or adult humor.\n    - Whimsical, or playful themes.\n    - Non-violent and simplified gameplay or narratives.\n    - Content with rhythmic elements (e.g., chants, exclamations, or songs appealing to young viewers).\n\n    Important Considerations:\n    - Content can be child-directed even if it also appeals to older audiences.\n    - Gaming content is child-directed if it features family-friendly gameplay and cartoonish visuals.\n    - Movie/TV clips are child-directed if rated G/PG.\n    - Presence of mild peril or conflict is acceptable if presented appropriately.\n\n    Exclusions:\n    - Do not label content as \"Child Directed\" if it features significant:\n      - Dark, mature, or violent themes.\n      - Romantic subplots or adult humor.\n      - Complex dialogue or advanced vocabulary inappropriate for children.\n      - Songs that are not nursery rhymes or explicitly created for children.\n    - Recipe, DIY, or instructional videos unless simplified for children (e.g., \"Cooking for Kids\").\n\n    Instructions:\n    Labeling:\n    If the video meets the child-directed criteria label it as \"Child Directed\".\n    If it does not meet these criteria label it as \"Not Child Directed\".\n\n    Indicate the spoken language if any.\n    Provide a brief justification explaining why the video is considered child-directed or not.\n\n    Make the outputs in JSON format (keys inlcude 'label' (only one of: 'Child Directed', or 'Not Child Directed'), 'justification', and 'language' (if any is spoken)).\n  Please say nothing else outside of this json format. \n  \"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:55:20.287595Z","iopub.execute_input":"2025-01-10T08:55:20.287956Z","iopub.status.idle":"2025-01-10T08:55:20.292234Z","shell.execute_reply.started":"2025-01-10T08:55:20.287933Z","shell.execute_reply":"2025-01-10T08:55:20.291403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ids_covered = ['none']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:55:23.879288Z","iopub.execute_input":"2025-01-10T08:55:23.879605Z","iopub.status.idle":"2025-01-10T08:55:23.882748Z","shell.execute_reply.started":"2025-01-10T08:55:23.879584Z","shell.execute_reply":"2025-01-10T08:55:23.882107Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Running Model on Dataset** ","metadata":{"id":"w8esg7q8hSTW"}},{"cell_type":"code","source":"ids = []\nlabels = []\nresponses = []\npredicted_labels = [] \nlanguages= [] \nremaining = [] \n\nimg_dir = '/kaggle/working/Images'\nif not os.path.exists(img_dir):\n    os.makedirs(img_dir)\n\nfor i in range(len(paths)): \n    \n    if video_ids[i] not in ids_covered:\n        try:\n            contents_of_ad = os.listdir(paths[i]) \n            contents_of_ad.remove('audio.mp3') \n            video_path = paths[i] + '/' + contents_of_ad[0] \n            audio_path = paths[i] + '/audio.mp3' \n    \n            try: \n                print(i, video_path)\n    \n                # Extract multiple images representative of the video\n                try:\n                    images = get_images(video_path)\n                except:\n                    images = get_images_2(video_path)\n    \n                # Save each image returned by extract_images_of_frames\n                image_paths = []\n                for idx, img in enumerate(images):\n                    # Convert NumPy array to PIL image\n                    image = Image.fromarray(img)\n                    \n                    # Save the image to a file \n                    image_name = f\"{video_ids[i]}_{idx + 1}.png\"\n                    image_path = os.path.join(img_dir, image_name)\n                    image.save(image_path)\n                    image_paths.append(image_path)\n                    \n                # Display the images\n                fig, axes = plt.subplots(1, len(images), figsize=(10, 3))\n                if len(images) == 1:\n                    axes.imshow(images[0])\n                    axes.axis('off')\n                else:\n                    for ax, img in zip(axes, images):\n                        ax.imshow(img)\n                        ax.axis('off')\n    \n                plt.tight_layout()\n                plt.show()\n    \n                audio_transcription = transcriptions[i]\n    \n                # Make inference\n                classification_response = classify_video_with_images(prompt, audio_transcription, image_paths)\n                temp_id = video_ids[i] \n                temp_label = primary_labels[i] \n                temp_response = classification_response.content[0].text \n                pred_temp = json.loads(classification_response.content[0].text).get('label') \n                lang_temp = json.loads(classification_response.content[0].text).get('language') \n    \n                print('Id:', temp_id, '. Primary Label:', temp_label, '\\nResponse:', temp_response) \n    \n                ids.append(temp_id) \n                labels.append(temp_label) \n                responses.append(temp_response) \n                predicted_labels.append(pred_temp) \n                languages.append(lang_temp) \n                \n            except: \n                print('failed for ', i) \n                remaining.append(video_ids[i])\n        except: \n            print('path not found ', i) \n            remaining.append(video_ids[i])\n\n        time.sleep(20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:56:31.683695Z","iopub.execute_input":"2025-01-10T08:56:31.684024Z","iopub.status.idle":"2025-01-10T08:57:38.368460Z","shell.execute_reply.started":"2025-01-10T08:56:31.684004Z","shell.execute_reply":"2025-01-10T08:57:38.367484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(ids) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:57:45.794452Z","iopub.execute_input":"2025-01-10T08:57:45.794763Z","iopub.status.idle":"2025-01-10T08:57:45.800051Z","shell.execute_reply.started":"2025-01-10T08:57:45.794743Z","shell.execute_reply":"2025-01-10T08:57:45.798945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# At the end, print remaining videos \n\nprint(\"Remaining videos with errors:\", remaining) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:57:46.017742Z","iopub.execute_input":"2025-01-10T08:57:46.018054Z","iopub.status.idle":"2025-01-10T08:57:46.022018Z","shell.execute_reply.started":"2025-01-10T08:57:46.018034Z","shell.execute_reply":"2025-01-10T08:57:46.021244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(responses)): \n    print('True Label: ', labels[i], '\\tPrediction: ', predicted_labels[i]) ","metadata":{"id":"aqDlWLZcZJHL","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:57:48.085319Z","iopub.execute_input":"2025-01-10T08:57:48.085628Z","iopub.status.idle":"2025-01-10T08:57:48.091569Z","shell.execute_reply.started":"2025-01-10T08:57:48.085600Z","shell.execute_reply":"2025-01-10T08:57:48.090408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df = pd.DataFrame({\n    'Video Id': ids,\n    'Primary Label': labels,\n    'Predicted Label': predicted_labels,\n    'Languages': languages, \n    'Response': responses \n})\n\nnew_df.head() ","metadata":{"id":"6JsiW7gbvxRA","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:57:48.308489Z","iopub.execute_input":"2025-01-10T08:57:48.308821Z","iopub.status.idle":"2025-01-10T08:57:48.323582Z","shell.execute_reply.started":"2025-01-10T08:57:48.308800Z","shell.execute_reply":"2025-01-10T08:57:48.322560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_dir = '/kaggle/working/results'\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:57:52.128626Z","iopub.execute_input":"2025-01-10T08:57:52.129019Z","iopub.status.idle":"2025-01-10T08:57:52.134648Z","shell.execute_reply.started":"2025-01-10T08:57:52.128990Z","shell.execute_reply":"2025-01-10T08:57:52.133071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df.to_csv('/kaggle/working/results/DAVSP_Claude-3.5.csv', index=False)","metadata":{"id":"Tv1_5sfJwRfu","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:57:52.348847Z","iopub.execute_input":"2025-01-10T08:57:52.349263Z","iopub.status.idle":"2025-01-10T08:57:52.358109Z","shell.execute_reply.started":"2025-01-10T08:57:52.349234Z","shell.execute_reply":"2025-01-10T08:57:52.357115Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Changing to binary lists \n\npredictions = [1 if pred == 'Not Child Directed' else 0 for pred in predicted_labels] \nground_truths = [1 if label == 'Not Child Directed' else 0 for label in labels] ","metadata":{"id":"4hsLj8P8Mg-e","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:57:55.831695Z","iopub.execute_input":"2025-01-10T08:57:55.832061Z","iopub.status.idle":"2025-01-10T08:57:55.836958Z","shell.execute_reply.started":"2025-01-10T08:57:55.832040Z","shell.execute_reply":"2025-01-10T08:57:55.835416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Obtaining classification report \nfrom sklearn.metrics import classification_report \n\nreport = classification_report(ground_truths, predictions) \nprint(report) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:57:56.075033Z","iopub.execute_input":"2025-01-10T08:57:56.075327Z","iopub.status.idle":"2025-01-10T08:57:56.092705Z","shell.execute_reply.started":"2025-01-10T08:57:56.075307Z","shell.execute_reply":"2025-01-10T08:57:56.091240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(ground_truths, predictions)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Child Directed', 'Child Directed'], yticklabels=['Not Child Directed', 'Child Directed'])\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T06:26:01.458638Z","iopub.execute_input":"2025-01-08T06:26:01.459059Z","iopub.status.idle":"2025-01-08T06:26:01.760378Z","shell.execute_reply.started":"2025-01-08T06:26:01.459025Z","shell.execute_reply":"2025-01-08T06:26:01.758914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}