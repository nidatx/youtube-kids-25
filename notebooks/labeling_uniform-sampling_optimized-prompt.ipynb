{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10328861,"sourceType":"datasetVersion","datasetId":6395471},{"sourceId":10349036,"sourceType":"datasetVersion","datasetId":6296913},{"sourceId":10358109,"sourceType":"datasetVersion","datasetId":5987034},{"sourceId":10549716,"sourceType":"datasetVersion","datasetId":6179624},{"sourceId":10550838,"sourceType":"datasetVersion","datasetId":6411015}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"id":"exmY-abTefFW"}},{"cell_type":"code","source":"!pip install ffmpeg-python\n!pip install av\n!pip install -q -U google-generativeai\n!pip install --upgrade pip\n!pip install --upgrade transformers datasets[audio] accelerate\n!pip install scenedetect","metadata":{"id":"Bi3c0wLhh-Rf","outputId":"44c463e5-7c4c-45cf-daca-75620249739c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport time\nimport torch\nimport random\nimport ffmpeg\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom glob import glob\nimport soundfile as sf\nfrom json import loads,dumps\n#from pydub import AudioSegment\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom scipy.signal import resample\nimport typing_extensions as typing\nfrom google.generativeai.types import HarmCategory, HarmBlockThreshold\nfrom scenedetect import open_video, VideoStreamCv2, SceneManager\nfrom scenedetect.detectors import ContentDetector","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"limit = 5000 ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"# Setting up video directories \n\ntrue1dir = '/kaggle/input/video-classification-data/Videos/Videos/videos-true' \ntrue2dir = '/kaggle/input/video-classification-data/Videos/Videos/videos-true-2' \nkids_dir = '/kaggle/input/video-classification-data/Made-For-Kids/Made-For-Kids'\n\nfalse1dir = '/kaggle/input/video-classification-data/Videos/Videos/videos-false' \nfalse2dir = '/kaggle/input/video-classification-data/Videos/Videos/videos-false-2' \nnon_kids_dir = '/kaggle/input/video-classification-data/Non Made For Kids/Non Made For Kids'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Functions to extract corresponding transcriptions and ground truths \n\ndef get_transcriptions_and_paths(vids_dir, ground_truths, transcriptions_path): \n\n    ids = os.listdir(vids_dir) \n\n    labels_df = pd.DataFrame({ \n        'IDs': ids, \n        'Labels': [ground_truths] * len(ids) \n    }) \n    \n    transcriptions_df = pd.read_csv(transcriptions_path) \n    \n    # Merging dataframes \n    df = pd.merge(labels_df, transcriptions_df, left_on='IDs', right_on='Video Id') \n\n    # Extracting transcriptions \n    ids = list(df['IDs']) \n    paths = [vids_dir + '/' + id for id in ids] \n    all_transcriptions = list(df['Transcription']) \n\n    # Extracting data from transcripts \n    transcriptions_temp = [] \n    lengths = [] \n    for (i, id_) in enumerate(ids): \n        transcriptions_temp.append(all_transcriptions[i].split(\"chunks\")[0]) \n        lengths.append(len(all_transcriptions[i].split(\"chunks\")[0])) \n\n    # setting limit on transcription length \n    transcriptions = [x[:limit] + '...' if len(x) > limit else x for x in transcriptions_temp] \n\n    return paths, transcriptions, [ground_truths] * len(ids) ","metadata":{"id":"1ksUpR0ibKve","outputId":"6880ee21-f703-4761-dfcd-842e1f8b61d9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading paths, transcriptions, and labels \n\ntrue1_paths, true1_transcriptions, true1_labels = get_transcriptions_and_paths(true1dir, 'Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/true-1-translated-transcriptions.csv') \ntrue2_paths, true2_transcriptions, true2_labels = get_transcriptions_and_paths(true2dir, 'Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/true-2-translated-transcriptions.csv') \nkids_paths, kids_transcriptions, kids_labels = get_transcriptions_and_paths(kids_dir, 'Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/made-for-kids_translated_transcriptions.csv') \n\nfalse1_paths, false1_transcriptions, false1_labels = get_transcriptions_and_paths(false1dir, 'Not Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/false-1-translated-transcriptions.csv') \nfalse2_paths, false2_transcriptions, false2_labels = get_transcriptions_and_paths(false2dir, 'Not Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/false-2-translated-transcriptions.csv') \nnon_kids_paths, non_kids_transcriptions, non_kids_labels = get_transcriptions_and_paths(non_kids_dir, 'Not Child Directed', '/kaggle/input/video-classification-data/Translated Transcriptions/Translated Transcriptions/non-made-for-kids_translated_transcriptions.csv') ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final combined list of paths and ground labels \n\npaths = [] \ntranscriptions = [] \nprimary_labels = [] \n\npaths.extend(false1_paths) \npaths.extend(true1_paths) \npaths.extend(false2_paths) \npaths.extend(true2_paths) \npaths.extend(non_kids_paths)\npaths.extend(kids_paths) \n\nprimary_labels.extend(false1_labels) \nprimary_labels.extend(true1_labels) \nprimary_labels.extend(false2_labels) \nprimary_labels.extend(true2_labels) \nprimary_labels.extend(non_kids_labels) \nprimary_labels.extend(kids_labels) \n\ntranscriptions.extend(false1_transcriptions) \ntranscriptions.extend(true1_transcriptions) \ntranscriptions.extend(false2_transcriptions) \ntranscriptions.extend(true2_transcriptions) \ntranscriptions.extend(non_kids_transcriptions) \ntranscriptions.extend(kids_transcriptions) \n\nvideo_ids = [path.split('/')[-1] for path in paths]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(paths), len(primary_labels), len(transcriptions), len(video_ids)) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Extracting Images** ","metadata":{}},{"cell_type":"code","source":"def extract_single_image(video_path, num_frames=6):\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Calculate the interval at which to sample frames\n    interval = max(1, total_frames // num_frames)\n    frames = []\n\n    # Extract frames at regular intervals\n    for i in range(num_frames):\n        frame_no = i * interval\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n        ret, frame = cap.read()\n        if ret:\n            frames.append(frame)\n\n    cap.release()\n\n    # Ensure the correct number of frames were extracted\n    if len(frames) < num_frames:\n        raise ValueError(\"Not enough frames extracted from the video.\")\n\n    # Concatenate all frames into a single image\n    try:\n        concatenated_rows = []\n        for i in range(0, num_frames, 2):\n            row = np.concatenate(frames[i:i+2], axis=1)\n            concatenated_rows.append(row)\n        concatenated_image = np.concatenate(concatenated_rows, axis=0)\n    except Exception as e:\n        raise ValueError(f\"Error during frame concatenation: {e}\")\n\n    # Resize the final image\n    final_image = Image.fromarray(concatenated_image).resize((1000, 1000))\n\n    return np.array(final_image)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Defining Model** ","metadata":{}},{"cell_type":"code","source":"key_ = '' # add key here ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import google.generativeai as genai \nos.environ[\"API_KEY\"] = key \ngenai.configure(api_key=os.environ[\"API_KEY\"]) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = genai.GenerativeModel(\"gemini-1.5-flash\", system_instruction=\"You are an expert content moderator.\") ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LLM_Output(typing.TypedDict):\n    label: str\n    language: list[str]\n    response: str","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Prompt** ","metadata":{}},{"cell_type":"code","source":"# Create the prompt.\nprompt = \"\"\"\n    A piece of content is child-directed if it meets any of these criteria:\n\n    Designed for Children:\n    The content is explicitly created with children as the intended audience, such as:\n      - Educational videos for kids.\n      - Child-friendly video games with cartoonish graphics and non-violent gameplay and story-telling\n      - Animated movies/clips suitable for family viewing\n      - Simple crafts, activities, or demonstrations aimed at children.\n\n    Child-Appealing Elements:\n    The content includes features commonly enjoyed by children, such as:\n    - Colorful, cartoonish, or animated visuals (e.g., animals, anthropomorphic characters, fantasy creatures).\n    - Light-hearted portrayals of themes like mischief or conflict, but without mature visuals, strong language, or adult humor.\n    - Whimsical, or playful themes.\n    - Non-violent and simplified gameplay or narratives.\n    - Content with rhythmic elements (e.g., chants, exclamations, or songs appealing to young viewers).\n\n    Important Considerations:\n    - Content can be child-directed even if it also appeals to older audiences.\n    - Gaming content is child-directed if it features family-friendly gameplay and cartoonish visuals.\n    - Movie/TV clips are child-directed if rated G/PG.\n    - Presence of mild peril or conflict is acceptable if presented appropriately.\n\n    Exclusions:\n    - Do not label content as \"Child Directed\" if it features significant:\n      - Dark, mature, or violent themes.\n      - Romantic subplots or adult humor.\n      - Complex dialogue or advanced vocabulary inappropriate for children.\n      - Songs that are not nursery rhymes or explicitly created for children.\n    - Recipe, DIY, or instructional videos unless simplified for children (e.g., \"Cooking for Kids\").\n\n    Instructions:\n    Labeling:\n    If the video meets the child-directed criteria label it as \"Child Directed\".\n    If it does not meet these criteria label it as \"Not Child Directed\".\n\n    Indicate the spoken language if any.\n    Provide a brief justification explaining why the video is considered child-directed or not.\n\n    Format the output in JSON.\n  \"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Running Model on Dataset** ","metadata":{"id":"w8esg7q8hSTW"}},{"cell_type":"code","source":"ids = []\npredicted_labels = []\nlanguages = []\nresponses = []\nground_truths = []\nremaining = []\n\nimg_dir = '/kaggle/working/Images'\nif not os.path.exists(img_dir):\n    os.makedirs(img_dir)\n\nfor i in range(len(paths)): \n    try:\n        contents_of_ad = os.listdir(paths[i]) \n        contents_of_ad.remove('audio.mp3') \n        video_path = paths[i] + '/' + contents_of_ad[0] \n        audio_path = paths[i] + '/audio.mp3' \n\n        # Extract a grid image representative of entire video\n        grid = extract_single_image(video_path)\n\n        # Convert NumPy array to PIL image\n        image = Image.fromarray(grid)\n\n        # Save the image to a file\n        image_name = video_ids[i] + \".png\"\n        image_path = os.path.join(img_dir, image_name)\n        image.save(image_path)\n        \n        # Display the image\n        plt.imshow(image)\n        plt.axis('off')\n        plt.tight_layout()\n        plt.show()\n\n        # Wrap image upload in try-except\n        try:\n            image_file = genai.upload_file(path = image_path, resumable=False)\n        except Exception as e:\n            print(f\"Error uploading image: {e}\")\n            remaining.append(video_ids[i])\n            continue\n\n         # Check if image has uploaded\n        try:\n            while image_file.state.name == \"PROCESSING\":\n                print('.', end='')\n                time.sleep(10)\n                image_file = genai.get_file(image_file.name)\n\n            if image_file.state.name == \"FAILED\":\n                raise ValueError(image_file.state.name)\n        except Exception as e:\n            print(f\"Error during image processing: {e}\")\n            remaining.append(video_ids[i])\n            continue\n\n        # Make inference with audio and image URIs\n        audio = transcriptions[i]\n        try: \n            response = model.generate_content([audio, image_file, prompt],\n                                              generation_config=genai.GenerationConfig(\n                                                  response_mime_type=\"application/json\",\n                                                  response_schema=LLM_Output, \n                                                  temperature=0.0), \n                                              safety_settings={\n                                                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n                                                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n                                                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n                                                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n                                              })\n        except Exception as e:\n            print(f\"Error making inference: {e}\")\n            remaining.append(video_ids[i])\n            continue\n\n        # Wrap response.text access in try-except\n        try:\n            print(\"Completed for video number:\", i, ' ', video_ids[i])\n\n            dictionary = loads(response.text)\n            print('True Label:', primary_labels[i], 'Response:', dictionary)\n\n            ids.append(video_ids[i])\n            predicted_labels.append(dictionary['label'])\n            languages.append(dictionary['language'])\n            responses.append(dictionary['response'])\n            ground_truths.append(primary_labels[i]) \n            \n        except Exception as e:\n            print(f\"Error processing response.text: {e}\")\n            remaining.append(video_ids[i])\n            continue\n\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        remaining.append(video_ids[i])\n        continue\n\n    time.sleep(20)","metadata":{"id":"cSoT7-9DDknR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(ids)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# At the end, print remaining videos \n\nprint(\"Remaining videos with errors:\", remaining) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(responses)): \n    print('True Label: ', ground_truths[i], '\\tPrediction: ', predicted_labels[i]) ","metadata":{"id":"aqDlWLZcZJHL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df = pd.DataFrame({\n    'Video Id': ids,\n    'Primary Label': ground_truths,\n    'Predicted Label': predicted_labels,\n    'Response': responses, \n    'Languages': languages \n})\n\nnew_df.head() ","metadata":{"id":"6JsiW7gbvxRA","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_dir = '/kaggle/working/results'\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df.to_csv('/kaggle/working/results/uniform-sampling+transcriptions_optimized-prompt.csv', index=False)","metadata":{"id":"Tv1_5sfJwRfu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Changing to binary lists \n\npredictions = [1 if pred == 'Not Child Directed' else 0 for pred in predicted_labels] \nground_truths = [1 if label == 'Not Child Directed' else 0 for label in ground_truths] ","metadata":{"id":"4hsLj8P8Mg-e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Obtaining classification report \nfrom sklearn.metrics import classification_report \n\nreport = classification_report(ground_truths, predictions) \nprint(report) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(ground_truths, predictions)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Child Directed', 'Child Directed'], yticklabels=['Not Child Directed', 'Child Directed'])\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}